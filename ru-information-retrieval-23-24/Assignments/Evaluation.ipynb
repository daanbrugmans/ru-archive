{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e91c5b41",
      "metadata": {
        "id": "e91c5b41"
      },
      "source": [
        "# Assignment 3: Evaluation\n",
        "\n",
        "In this assignment you will implement various ranking metrics to evaluate rankings produced by three ranking systems.\n",
        "\n",
        "We highly recommend that you use [Google Colab](https://colab.research.google.com/) for this assignment.\n",
        "\n",
        "\n",
        "## 1. Download the QREL and result files\n",
        "Start by downloading the QREL file and three result files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "kYdW59Qo0PnA",
      "metadata": {
        "id": "kYdW59Qo0PnA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-04 14:54:15--  http://gem.cs.ru.nl/IR-Course/qrel.txt\n",
            "Resolving gem.cs.ru.nl (gem.cs.ru.nl)... 131.174.31.31\n",
            "Connecting to gem.cs.ru.nl (gem.cs.ru.nl)|131.174.31.31|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13164624 (13M) [text/plain]\n",
            "Saving to: ‘Data/qrel.txt’\n",
            "\n",
            "Data/qrel.txt       100%[===================>]  12.55M  20.8MB/s    in 0.6s    \n",
            "\n",
            "2023-10-04 14:54:16 (20.8 MB/s) - ‘Data/qrel.txt’ saved [13164624/13164624]\n",
            "\n",
            "--2023-10-04 14:54:16--  http://gem.cs.ru.nl/IR-Course/results_1.txt\n",
            "Resolving gem.cs.ru.nl (gem.cs.ru.nl)... 131.174.31.31\n",
            "Connecting to gem.cs.ru.nl (gem.cs.ru.nl)|131.174.31.31|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22823425 (22M) [text/plain]\n",
            "Saving to: ‘Data/results_1.txt’\n",
            "\n",
            "Data/results_1.txt  100%[===================>]  21.77M  27.1MB/s    in 0.8s    \n",
            "\n",
            "2023-10-04 14:54:17 (27.1 MB/s) - ‘Data/results_1.txt’ saved [22823425/22823425]\n",
            "\n",
            "--2023-10-04 14:54:17--  http://gem.cs.ru.nl/IR-Course/results_2.txt\n",
            "Resolving gem.cs.ru.nl (gem.cs.ru.nl)... 131.174.31.31\n",
            "Connecting to gem.cs.ru.nl (gem.cs.ru.nl)|131.174.31.31|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23421093 (22M) [text/plain]\n",
            "Saving to: ‘Data/results_2.txt’\n",
            "\n",
            "Data/results_2.txt  100%[===================>]  22.34M  27.2MB/s    in 0.8s    \n",
            "\n",
            "2023-10-04 14:54:18 (27.2 MB/s) - ‘Data/results_2.txt’ saved [23421093/23421093]\n",
            "\n",
            "--2023-10-04 14:54:18--  http://gem.cs.ru.nl/IR-Course/results_3.txt\n",
            "Resolving gem.cs.ru.nl (gem.cs.ru.nl)... 131.174.31.31\n",
            "Connecting to gem.cs.ru.nl (gem.cs.ru.nl)|131.174.31.31|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23530996 (22M) [text/plain]\n",
            "Saving to: ‘Data/results_3.txt’\n",
            "\n",
            "Data/results_3.txt  100%[===================>]  22.44M  20.2MB/s    in 1.1s    \n",
            "\n",
            "2023-10-04 14:54:19 (20.2 MB/s) - ‘Data/results_3.txt’ saved [23530996/23530996]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://gem.cs.ru.nl/IR-Course/qrel.txt -O Data/qrel.txt\n",
        "!wget http://gem.cs.ru.nl/IR-Course/results_1.txt -O Data/results_1.txt\n",
        "!wget http://gem.cs.ru.nl/IR-Course/results_2.txt -O Data/results_2.txt\n",
        "!wget http://gem.cs.ru.nl/IR-Course/results_3.txt -O Data/results_3.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861ca8db",
      "metadata": {
        "id": "861ca8db"
      },
      "source": [
        "### Allowed Packages\n",
        "For this assignment you are only allowed to use the following packages, you are **not allowed** to use any external existing implementations for reading QREL or results files or computing any of the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c25312db",
      "metadata": {
        "id": "c25312db"
      },
      "outputs": [],
      "source": [
        "# NOTE: I altered some of the paths in this notebook as to fit my local file hierarchy.\n",
        "    # The functionality of the program should be unaltered.\n",
        "\n",
        "import json\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78940edc",
      "metadata": {
        "id": "78940edc"
      },
      "source": [
        "## 2. Meet the data \n",
        "\n",
        "### 2.1 The QREL file\n",
        "\n",
        "The ground-truth containing query-document relevance judgments are provided in the qrel format, in the *qrel.txt* file.\n",
        "From the official [TREC website](https://trec.nist.gov/data/qrels_noneng/) we get the following description of the format:\n",
        "> The format of a qrels file is as follows:\n",
        "> \n",
        "> TOPIC       ITERATION       DOCUMENT#       RELEVANCY\n",
        "> \n",
        "> where TOPIC is the topic number,<br>\n",
        "> ITERATION is the feedback iteration (almost always zero and not used),<br>\n",
        "> DOCUMENT# is the official document number that corresponds to the \"docno\" field in the documents, and<br>\n",
        "> RELEVANCY is a binary code of 0 for not relevant and 1 for relevant.\n",
        "> \n",
        "> Sample Qrels File:\n",
        "> \n",
        "> 1 0 AP880212-0161 0<br>\n",
        "> 1 0 AP880216-0139 1<br>\n",
        "> 1 0 AP880216-0169 0<br>\n",
        "> 1 0 AP880217-0026 0<br>\n",
        "> 1 0 AP880217-0030 0\n",
        "\n",
        "For this assignment,\n",
        "the TOPIC number will indicate a query ID number,\n",
        "the ITERATION number can be ignored,\n",
        "DOCUMENT# provides an identifier for the document required for referencing with the result files,\n",
        "lastly RELEVANCY provides a binary number indicating relevancy with 0 for not relevant and 1 for relevant.\n",
        "\n",
        "Each line in the file represents the placement of a single document-query combination.\n",
        "Lines are ordered by query and thus can be parsed sequentially, the file starts with all documents for query 1 then 2, then 3 etc. Thus when query $N$ appears you can be sure that all of query $N-1$ has been parsed.\n",
        "\n",
        "Let's take a look at the first 30 lines of the *qrel.txt* file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d6d8573f",
      "metadata": {
        "id": "d6d8573f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 0 DOC9274 0 \n",
            "2 0 DOC1294 0 \n",
            "2 0 DOC3415 0 \n",
            "2 0 DOC5402 0 \n",
            "2 0 DOC6143 0 \n",
            "2 0 DOC3598 0 \n",
            "2 0 DOC0338 0 \n",
            "2 0 DOC8272 0 \n",
            "2 0 DOC1059 0 \n",
            "2 0 DOC1525 0 \n",
            "2 0 DOC8766 0 \n",
            "2 0 DOC2631 0 \n",
            "2 0 DOC4109 0 \n",
            "2 0 DOC1427 0 \n",
            "3 0 DOC5252 0 \n",
            "3 0 DOC8461 0 \n",
            "3 0 DOC1193 0 \n",
            "3 0 DOC6223 0 \n",
            "3 0 DOC0263 0 \n",
            "4 0 DOC6139 0 \n",
            "4 0 DOC6391 0 \n",
            "4 0 DOC5583 0 \n",
            "4 0 DOC5323 0 \n",
            "4 0 DOC8838 0 \n",
            "4 0 DOC6743 0 \n",
            "4 0 DOC6527 0 \n",
            "4 0 DOC0836 1 \n",
            "5 0 DOC1966 0 \n",
            "5 0 DOC9249 0 \n",
            "5 0 DOC0923 1 \n"
          ]
        }
      ],
      "source": [
        "with open('Data/qrel.txt', 'r') as qrel_file:\n",
        "    for _ in range(30):\n",
        "        print(next(qrel_file)[:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a52b53c",
      "metadata": {
        "id": "4a52b53c"
      },
      "source": [
        "We see that for the first query DOC9274 is the only annotated document and it is not considered relevant.\n",
        "\n",
        "Which documents are annotated per query is done via a pre-selection process. For the first query this resulted in a a single document, making it a *very* uninteresting query to rank for.\n",
        "\n",
        "While most document-query combinations are not considered relevant, we see that for query 4, DOC0836 is relevant and also for query 5 DOC0923 is relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0414d7bf",
      "metadata": {
        "id": "0414d7bf"
      },
      "source": [
        "### 2.2 The Results Files\n",
        "\n",
        "The rankings which you will evaluate are provided in three files representing the output of three different ranking systems: *results_1.txt*, *results_2.txt* and *results_3.txt*.\n",
        "\n",
        "In the last week's assignment you created similar result files yourself!\n",
        "\n",
        "Again each line in the file represents a single query-document combination and is formatted as: \n",
        "> TOPIC ITERATION DOCUMENT# RANK SCORE RUN#\n",
        "\n",
        "For this assignment, the TOPIC number will indicate a query ID number, the ITERATION number can be ignored, DOCUMENT# provides an identifier for the document required for referencing with the QREL file, RANK indicates at what rank this document was placed for this query, SCORE is the score the ranking model gave to the query-document combination (higher scores are ranked first), and lastly RUN# provides an identifier for the specific ranking system used. \n",
        "\n",
        "Let's take a look at the first 30 lines of the *results_1.txt* file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b0b95bcd",
      "metadata": {
        "id": "b0b95bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 0 DOC9274 1 0.37270 RUN1 \n",
            "2 0 DOC1294 8 0.37851 RUN1 \n",
            "2 0 DOC3415 9 0.36128 RUN1 \n",
            "2 0 DOC5402 5 0.39396 RUN1 \n",
            "2 0 DOC6143 7 0.37981 RUN1 \n",
            "2 0 DOC3598 6 0.38486 RUN1 \n",
            "2 0 DOC0338 10 0.35127 RUN1 \n",
            "2 0 DOC8272 12 0.34495 RUN1 \n",
            "2 0 DOC1059 1 0.42166 RUN1 \n",
            "2 0 DOC1525 2 0.41221 RUN1 \n",
            "2 0 DOC8766 11 0.35098 RUN1 \n",
            "2 0 DOC2631 3 0.39771 RUN1 \n",
            "2 0 DOC4109 13 0.33992 RUN1 \n",
            "2 0 DOC1427 4 0.39666 RUN1 \n",
            "3 0 DOC5252 2 0.37440 RUN1 \n",
            "3 0 DOC8461 1 0.39712 RUN1 \n",
            "3 0 DOC1193 3 0.36817 RUN1 \n",
            "3 0 DOC6223 5 0.35332 RUN1 \n",
            "3 0 DOC0263 4 0.36205 RUN1 \n",
            "4 0 DOC6139 5 0.37846 RUN1 \n",
            "4 0 DOC6391 8 0.35296 RUN1 \n",
            "4 0 DOC5583 4 0.38296 RUN1 \n",
            "4 0 DOC5323 1 0.42094 RUN1 \n",
            "4 0 DOC8838 2 0.40406 RUN1 \n",
            "4 0 DOC6743 6 0.37418 RUN1 \n",
            "4 0 DOC6527 7 0.36155 RUN1 \n",
            "4 0 DOC0836 3 0.39487 RUN1 \n",
            "5 0 DOC1966 7 0.41868 RUN1 \n",
            "5 0 DOC9249 13 0.38231 RUN1 \n",
            "5 0 DOC0923 5 0.42748 RUN1 \n"
          ]
        }
      ],
      "source": [
        "with open('Data/results_1.txt', 'r') as results_file:\n",
        "    for _ in range(30):\n",
        "        print(next(results_file)[:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cdfdfe8",
      "metadata": {
        "id": "0cdfdfe8"
      },
      "source": [
        "We see that for query 2 the first system provided the following ranking:\n",
        "> DOC1059 DOC1525 DOC2631 DOC1427 DOC5402 DOC3598 DOC6143 DOC1294 DOC3415 DOC0338 DOC8766 DOC8272 DOC4109\n",
        "\n",
        "For query 4 it has provided:\n",
        "> DOC5323 DOC8838 DOC0836 DOC5583 DOC6743 DOC6527 DOC6391\n",
        "\n",
        "Remember that in the QREL file, we saw that for query 4, DOC0836 was the only document annotated as relevant.\n",
        "Therefore we know that, in this ranking for query 4, the only relevant document is at rank 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d48c1046",
      "metadata": {
        "id": "d48c1046"
      },
      "source": [
        "## 3. Processing the Files\n",
        "\n",
        "### 3.1 Process a QREL Line\n",
        "You will start by implementing a function that parses a single line from the QREL file and outputs the query ID, the document ID and the relevancy of the document to the query.\n",
        "\n",
        "For instance, the following line:\n",
        ">4 0 DOC0836 1\n",
        "\n",
        "Should output the following tuple:\n",
        "> (4, 'DOC0836', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ecb66fa",
      "metadata": {
        "id": "1ecb66fa"
      },
      "outputs": [],
      "source": [
        "def parse_qrel_line(line):\n",
        "    # =======Your code=======\n",
        "    query, _, document, relevancy = line.strip().split(\" \")\n",
        "    query = int(query)\n",
        "    relevancy = int(relevancy)\n",
        "    # =======================\n",
        "    return query, document, relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c7a999",
      "metadata": {
        "id": "88c7a999"
      },
      "source": [
        "The following assertions provide some test cases to verify your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "416c5b68",
      "metadata": {
        "id": "416c5b68"
      },
      "outputs": [],
      "source": [
        "assert parse_qrel_line('1 0 DOC0001 0')[0] == 1\n",
        "assert parse_qrel_line('123 0 DOC0001 0')[0] == 123\n",
        "assert parse_qrel_line('1 0 DOC0001 0')[1] == 'DOC0001'\n",
        "assert parse_qrel_line('1 0 DOC1234 0')[1] == 'DOC1234'\n",
        "assert parse_qrel_line('1 0 DOC1234 0')[2] == 0\n",
        "assert parse_qrel_line('1 0 DOC1234 1')[2] == 1\n",
        "assert type(parse_qrel_line('1 0 DOC0001 0')[0]) == int\n",
        "assert type(parse_qrel_line('1 0 DOC0001 0')[2]) == int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff2921f8",
      "metadata": {
        "id": "ff2921f8"
      },
      "source": [
        "### 3.2 Process a Result Line\n",
        "Next implement a function that parses a single line from the results file and outputs the query ID, the document ID and the rank at which the document is placed by the ranking system.\n",
        "\n",
        "For instance, the following line:\n",
        ">2 0 DOC1294 8 0.37851 RUN1 \n",
        "\n",
        "Should output the following tuple:\n",
        "> (2, 'DOC1294', 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7377daf9",
      "metadata": {
        "id": "7377daf9"
      },
      "outputs": [],
      "source": [
        "def parse_results_line(line):\n",
        "    # =======Your code=======\n",
        "    query, _, document, rank, _, _ = line.strip().split(\" \")\n",
        "    query = int(query)\n",
        "    rank = int(rank)\n",
        "    # =======================\n",
        "    return query, document, rank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e106d9b2",
      "metadata": {
        "id": "e106d9b2"
      },
      "source": [
        "Again, here are several assertions to test your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "28941fdf",
      "metadata": {
        "id": "28941fdf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "assert parse_results_line('1 0 DOC0001 1 0.37270 RUN1')[0] == 1\n",
        "assert parse_results_line('123 0 DOC0001 1 0.37270 RUN1')[0] == 123\n",
        "assert parse_results_line('1 0 DOC0001 1 0.37270 RUN1')[1] == 'DOC0001'\n",
        "assert parse_results_line('1 0 DOC1234 1 0.37270 RUN1')[1] == 'DOC1234'\n",
        "assert parse_results_line('1 0 DOC1234 1 0.37270 RUN1')[2] == 1\n",
        "assert parse_results_line('1 0 DOC1234 123 0.37270 RUN1')[2] == 123\n",
        "assert type(parse_results_line('1 0 DOC0001 1 0.37270 RUN1')[0]) == int\n",
        "assert type(parse_results_line('1 0 DOC0001 1 0.37270 RUN1')[2]) == int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069496fb",
      "metadata": {
        "id": "069496fb"
      },
      "source": [
        "### 3.3 Storing Document-Query Relevancies\n",
        "\n",
        "In order to match the results and QREL files, it will be useful to have a lookup for the relevancies of document-query pairs according to the QREL file.\n",
        "\n",
        "Complete the simple class below, so that you can quickly look up whether a document is relevant to a query or not.\n",
        "\n",
        "*Hint: Use a dictionary of dictionary for `self.relevancies`; i.e., `{q1:{d1:r1, d2:r2, ...}, q2: {...}, ...}`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "28128601",
      "metadata": {
        "id": "28128601"
      },
      "outputs": [],
      "source": [
        "class relevancy_lookup(object):\n",
        "    def __init__(self):\n",
        "        self.relevancies = {}\n",
        "    \n",
        "    def add(self, query, document, relevancy):\n",
        "        # =======Your code=======            \n",
        "        if query not in self.relevancies:\n",
        "            self.relevancies[query] = {}\n",
        "        \n",
        "        self.relevancies[query][document] = relevancy\n",
        "        # =======================\n",
        "        \n",
        "    def get(self, query, document):\n",
        "        # =======Your code=======        \n",
        "        return self.relevancies[query][document]\n",
        "        # ======================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c8cabeb",
      "metadata": {
        "id": "4c8cabeb"
      },
      "source": [
        "The code below will store the relevancy of two documents w.r.t. two queries and then verify whether they were stored correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0790ce72",
      "metadata": {
        "id": "0790ce72"
      },
      "outputs": [],
      "source": [
        "test_lookup = relevancy_lookup()\n",
        "test_lookup.add(1, 'DOC0001', 1)\n",
        "test_lookup.add(2, 'DOC0001', 0)\n",
        "test_lookup.add(1, 'DOC1234', 0)\n",
        "test_lookup.add(2, 'DOC1234', 1)\n",
        "assert test_lookup.get(1, 'DOC0001') == 1\n",
        "assert test_lookup.get(2, 'DOC0001') == 0\n",
        "assert test_lookup.get(1, 'DOC1234') == 0\n",
        "assert test_lookup.get(2, 'DOC1234') == 1\n",
        "assert type(test_lookup.get(1, 'DOC0001')) == int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85aefdd5",
      "metadata": {
        "id": "85aefdd5"
      },
      "source": [
        "### 3.4 Matching Relevancy Labels with Results Rankings\n",
        "\n",
        "In order to compute IR metrics, we need to know at what ranks relevant documents where placed per query.\n",
        "\n",
        "This function takes as input, a *relevancy_lookup* as implemented above, a query ID number, and a list of documents and ranks, and as output it gives a numpy vector of relevancies, indicating the ranked labels.\n",
        "\n",
        "For example, *doc_rank_list* could be:\n",
        "> [('DOC0001', 1), ('DOC0002', 3), ('DOC0003', 2)]\n",
        "\n",
        "Indicating that DOC0001 is placed at rank 1, DOC0003 at rank 2 and DOC0002 at rank 3, i.e. the ranking:\n",
        "> [DOC0001, DOC0003, DOC0002]\n",
        "\n",
        "If DOC0001 and DOC0003 are relevant but DOC0002 is not then the output of the function should be:\n",
        ">[1,1,0]\n",
        "\n",
        "(stored as a numpy array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "52b4a70e",
      "metadata": {
        "id": "52b4a70e"
      },
      "outputs": [],
      "source": [
        "def get_ranked_labels(rel_lookup, query, doc_rank_list): \n",
        "    result = np.zeros(len(doc_rank_list), dtype=int)\n",
        "    for x in doc_rank_list:\n",
        "        result[x[1]-1] = rel_lookup.get(query, x[0])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8e2322",
      "metadata": {
        "id": "ae8e2322"
      },
      "source": [
        "### 3.5 Processing the QREL and Results Files\n",
        "\n",
        "Finally, it is time to combine the functions you have implemented above to process both the QREL and a result file at once.\n",
        "\n",
        "The function below will take as input a path to a QREL file and a results file and output the ranked labels as an iterable.\n",
        "This means that we can use this function in a for loop to easily iterate over all metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4646c23d",
      "metadata": {
        "id": "4646c23d"
      },
      "outputs": [],
      "source": [
        "def process_files(qrel_path, results_path):\n",
        "    relevancies = relevancy_lookup()\n",
        "    with open(qrel_path, 'r') as qrel_file:\n",
        "        for line in qrel_file:\n",
        "            query, document, relevancy = parse_qrel_line(line)\n",
        "            relevancies.add(query, document, relevancy)\n",
        "    with open(results_path, 'r') as results_file:\n",
        "        current_query, document, rank = parse_results_line(next(results_file))    \n",
        "        doc_rank_list = [(document, rank)]\n",
        "        for line in results_file:\n",
        "            query, document, rank = parse_results_line(line)\n",
        "            if query != current_query:\n",
        "                yield get_ranked_labels(relevancies, current_query, doc_rank_list)\n",
        "                current_query = query\n",
        "                doc_rank_list = [(document, rank)]\n",
        "            else:\n",
        "                doc_rank_list.append((document, rank))\n",
        "        yield get_ranked_labels(relevancies, current_query, doc_rank_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c46f44b",
      "metadata": {
        "id": "2c46f44b"
      },
      "source": [
        "For example, the code below should print the ranked labels for the first five queries (if you implemented the code in 3.1-3.3 correctly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a560d035",
      "metadata": {
        "id": "a560d035",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0]\n",
            "[0 0 1 0 0 0 0 0]\n",
            "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "counter = 5\n",
        "for x in process_files('Data/qrel.txt', 'Data/results_1.txt'):\n",
        "    print(x)\n",
        "    counter = counter - 1\n",
        "    if counter == 0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7089b877",
      "metadata": {
        "id": "7089b877"
      },
      "source": [
        "As a check, the code below will go over all queries, you should be able to run this code without error and in less than a minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "19812360",
      "metadata": {
        "id": "19812360",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for _ in process_files('Data/qrel.txt', 'Data/results_1.txt'):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04591539",
      "metadata": {
        "id": "04591539"
      },
      "source": [
        "## 4. Evaluation Metrics\n",
        "\n",
        "For this assignment, you will implement seven different ranking metrics.\n",
        "To keep descriptions exact we will provide formulas for the metric values for a single query, less formal descriptions can be found in the lecture slides.\n",
        "\n",
        "We will use the following notation:\n",
        "\n",
        "$K$ is the number of top results the metric considers, i.e. Precision@3 only looks at the top 3 results in the ranking, $r(q, i)$ indicates the label for the document ranked at position $i$ for query $q$ and  $D_q$ is the number of documents pre-selected for query $q$.\n",
        "\n",
        "Per query all the pre-selected documents are given in the result files so you do not have to consider the possibility of missing documents.\n",
        "\n",
        "Some queries will have less pre-selected documents than $K$ (e.g. query 1 only has one preselected document!), for this assignment, you should treat *no document* (missing documents) as a non-relevant document, i.e. $i > D_q \\rightarrow r(q, i) = 0$.\n",
        "\n",
        "If there are no relevant pre-selected documents for a query at all, all metric values are 0, this avoids any divisions by zero you may encounter.\n",
        "\n",
        "The metrics to implement are:\n",
        "1. The Precision metric:\n",
        "$$\\text{Precision}@K(q) = \\frac{1}{K} \\sum_{i=1}^K r(q, i).$$\n",
        "\n",
        "2. The Recall metric:\n",
        "$$\\text{Recall}@K(q) = \\frac{\\sum_{i=1}^K r(q, i)}{\\sum_{j=1}^{D_q} r(q, j)}.$$\n",
        "\n",
        "3. The F-Score metric using the harmonic mean (a.k.a. the F-Measure):\n",
        "$$\\text{F-Score}@K(q) =  \\frac{2 \\cdot \\text{Precision}@K \\cdot \\text{Recall}@K}{\\left(\\text{Precision}@K + \\text{Recall}@K\\right)}.$$\n",
        "\n",
        "4. Discounted Cumulative Gain (DCG):\n",
        "$$\n",
        "\\text{DCG}@K(q) = \\sum_{i=1}^{\\min(K, D_q)} \\frac{r(q, i)}{\\log_2(1+i)}.\n",
        "$$\n",
        "\n",
        "5. Normalized Discounted Cumulative Gain (NDCG) which is normalized by the Ideal DCG (IDCG), i.e. the maximum possible DCG value for the query:\n",
        "$$\n",
        "\\text{NDCG}@K(q) = \\frac{\\text{DCG}@K(q)}{\\text{IDCG}@K(q)}.\n",
        "$$\n",
        "\n",
        "6. The Average Precision (AP) which is based on the query-level Average Precision:\n",
        "$$\n",
        "\\text{Average-Precision}(q) = \\frac{\\sum_{k=1}^{D_q} r(q, k) \\cdot \\text{Precision}@k(q) }{\\sum_{i=1}^{D_q} r(q, i)}.\n",
        "$$\n",
        "\n",
        "7. The Reciprocal Rank (RR) which is based on the query-level Reciprocal Rank:\n",
        "$$\n",
        "\\text{Reciprocal Rank}(q) = \\frac{1}{\\min\\{i : r(q,i) = 1\\}}.\n",
        "$$\n",
        "\n",
        "**Note that when we take the avergae of AP and RR over all queries, we call the metrics Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), respectively.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8101741f",
      "metadata": {
        "id": "8101741f"
      },
      "source": [
        "## 5. Implement the Evaluation Metrics\n",
        "\n",
        "You will now implement each of these metrics, we have provided test cases for precision and recommend that you create your own tests for the other metrics.\n",
        "\n",
        "Section 6 will verify your implementation on one result file, you can use this as a strong correctness test of your implementation.\n",
        "\n",
        "We recommend using [basic Numpy operations](https://numpy.org/doc/stable/user/quickstart.html#basic-operations) to perform most math operations.\n",
        "\n",
        "### 5.1 Precision\n",
        "\n",
        "Implement the precision function below, the input is a numpy array of relevancies (i.e. [0,1,0,0,0]) and k, the number of top results the metric considers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5ee0adf1",
      "metadata": {
        "id": "5ee0adf1"
      },
      "outputs": [],
      "source": [
        "def precision(query_relevancy_labels, k):\n",
        "    # =======Your code=======\n",
        "    return 1/k * np.sum(query_relevancy_labels[:k])\n",
        "    # ======================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fefdae",
      "metadata": {
        "id": "d4fefdae"
      },
      "source": [
        "The tests below check your implementation on various example cases, note that we use [np.isclose](https://numpy.org/doc/stable/reference/generated/numpy.isclose.html) to do the [float comparisons](https://en.wikipedia.org/wiki/Round-off_error)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8aa7339f",
      "metadata": {
        "id": "8aa7339f"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(precision(np.array([0,0,0,0,0], dtype=int), 5), 0)\n",
        "assert np.isclose(precision(np.array([0,0,0,0,1], dtype=int), 5), 0.2)\n",
        "assert np.isclose(precision(np.array([0,0,0,0,1], dtype=int), 4), 0)\n",
        "assert np.isclose(precision(np.array([1,1,1,1,0], dtype=int), 4), 1)\n",
        "assert np.isclose(precision(np.array([1,1,1,1,1], dtype=int), 4), 1)\n",
        "assert np.isclose(precision(np.array([0,1,1,1,1], dtype=int), 4), 0.75)\n",
        "assert np.isclose(precision(np.array([0,0,1,1,1], dtype=int), 3), 0.333333333333333)\n",
        "assert np.isclose(precision(np.array([0], dtype=int), 5), 0)\n",
        "assert np.isclose(precision(np.array([1], dtype=int), 5), 0.2)\n",
        "assert np.isclose(precision(np.array([0, 1], dtype=int), 5), 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97fa9d48",
      "metadata": {
        "id": "97fa9d48"
      },
      "source": [
        "### 5.2 Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9b8794b5",
      "metadata": {
        "id": "9b8794b5"
      },
      "outputs": [],
      "source": [
        "def recall(query_relevancy_labels, k):\n",
        "    # =======Your code=======\n",
        "    sum_of_all_query_relevancy_labels = np.sum(query_relevancy_labels)\n",
        "    \n",
        "    if sum_of_all_query_relevancy_labels == 0:\n",
        "        return 0\n",
        "    \n",
        "    sum_of_query_relevancy_labels_at_k = np.sum(query_relevancy_labels[:k])\n",
        "\n",
        "    return sum_of_query_relevancy_labels_at_k / sum_of_all_query_relevancy_labels\n",
        "    # ======================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a06b36fb",
      "metadata": {
        "id": "a06b36fb"
      },
      "outputs": [],
      "source": [
        "# Test your code\n",
        "assert np.isclose(recall(np.array([0,0,0,0,0], dtype=int), 5), 0)\n",
        "assert np.isclose(recall(np.array([0,0,0,0,1], dtype=int), 5), 1)\n",
        "assert np.isclose(recall(np.array([0,0,0,0,1], dtype=int), 4), 0)\n",
        "assert np.isclose(recall(np.array([1,1,1,1,0], dtype=int), 4), 1)\n",
        "assert np.isclose(recall(np.array([1,1,1,1,1], dtype=int), 4), 0.8)\n",
        "assert np.isclose(recall(np.array([0,1,1,1,1], dtype=int), 4), 0.75)\n",
        "assert np.isclose(recall(np.array([0,0,1,1,1], dtype=int), 3), 0.333333333333333)\n",
        "assert np.isclose(recall(np.array([0], dtype=int), 5), 0)\n",
        "assert np.isclose(recall(np.array([1], dtype=int), 5), 1)\n",
        "assert np.isclose(recall(np.array([0, 1], dtype=int), 5), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb5118ba",
      "metadata": {
        "id": "bb5118ba"
      },
      "source": [
        "### 5.3 F-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2a4a2f37",
      "metadata": {
        "id": "2a4a2f37"
      },
      "outputs": [],
      "source": [
        "def F_score(query_relevancy_labels, k):\n",
        "    # =======Your code=======\n",
        "    sum_of_precision_and_recall_at_k = precision(query_relevancy_labels, k) + recall(query_relevancy_labels, k)\n",
        "    \n",
        "    if sum_of_precision_and_recall_at_k == 0:\n",
        "        return 0\n",
        "    \n",
        "    return 2 * precision(query_relevancy_labels, k) * recall(query_relevancy_labels, k) / sum_of_precision_and_recall_at_k\n",
        "    # ======================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "PfgxCatWnUxn",
      "metadata": {
        "id": "PfgxCatWnUxn"
      },
      "outputs": [],
      "source": [
        "# Test your code\n",
        "assert np.isclose(F_score(np.array([0,0,0,0,0], dtype=int), 5), 0)\n",
        "assert np.isclose(F_score(np.array([0,0,0,0,1], dtype=int), 5), 0.33333333333333337)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b56b3b3",
      "metadata": {
        "id": "5b56b3b3"
      },
      "source": [
        "### 5.4 Discounted Cumulative Gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "218321dc",
      "metadata": {
        "id": "218321dc"
      },
      "outputs": [],
      "source": [
        "def DCG(query_relevancy_labels, k):\n",
        "    # Use log with base 2\n",
        "    # =======Your code=======\n",
        "    document_count = query_relevancy_labels.size\n",
        "    lesser_of_k_and_document_count = np.min([k, document_count])    \n",
        "    dcg_at_k = 0\n",
        "    \n",
        "    for i in range(lesser_of_k_and_document_count):\n",
        "        if np.log2(1 + i + 1) == 0:\n",
        "            continue\n",
        "        \n",
        "        dcg_at_k += query_relevancy_labels[i] / np.log2(1 + i + 1)\n",
        "        \n",
        "    return dcg_at_k\n",
        "    # ======================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "UuLZNsgNtQYc",
      "metadata": {
        "id": "UuLZNsgNtQYc"
      },
      "outputs": [],
      "source": [
        "# Test your code\n",
        "assert np.isclose(DCG(np.array([0,0,0,0,0], dtype=int), 5), 0)\n",
        "assert np.isclose(DCG(np.array([0,0,0,0,1], dtype=int), 5), 0.38685280723454163)\n",
        "assert np.isclose(DCG(np.array([0,1,1,1,1], dtype=int), 4), 1.5616063116448506)\n",
        "assert np.isclose(DCG(np.array([0,0,1,1,1], dtype=int), 3), 0.5)\n",
        "assert np.isclose(DCG(np.array([0], dtype=int), 5), 0)\n",
        "assert np.isclose(DCG(np.array([1], dtype=int), 5), 1)\n",
        "assert np.isclose(DCG(np.array([0, 1], dtype=int), 5), 0.6309297535714575)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "becb86ba",
      "metadata": {
        "id": "becb86ba"
      },
      "source": [
        "### 5.5 Normalized Discounted Cumulative Gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9fd095aa",
      "metadata": {
        "id": "9fd095aa"
      },
      "outputs": [],
      "source": [
        "def NDCG(query_relevancy_labels, k):\n",
        "    # =======Your code=======   \n",
        "    query_relevancy_labels_optimal_order = -np.sort(-query_relevancy_labels)\n",
        "    optimal_idcg_at_k = DCG(query_relevancy_labels_optimal_order, k)\n",
        "    \n",
        "    if optimal_idcg_at_k == 0:\n",
        "        return 0\n",
        "            \n",
        "    return DCG(query_relevancy_labels, k) / optimal_idcg_at_k\n",
        "    # ======================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "vmRfjg8QvDNv",
      "metadata": {
        "id": "vmRfjg8QvDNv"
      },
      "outputs": [],
      "source": [
        "# Test your code\n",
        "assert np.isclose(NDCG(np.array([0,0,0,0,0], dtype=int), 5), 0)\n",
        "assert np.isclose(NDCG(np.array([0,0,0,0,1], dtype=int), 5), 0.38685280723454163)\n",
        "assert np.isclose(NDCG(np.array([0,1,1,1,1], dtype=int), 4), 0.6096199500078984)\n",
        "assert np.isclose(NDCG(np.array([0, 1], dtype=int), 5), 0.6309297535714575)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7018ba",
      "metadata": {
        "id": "1a7018ba"
      },
      "source": [
        "### 5.6 Average Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "97bd9106",
      "metadata": {
        "id": "97bd9106"
      },
      "outputs": [],
      "source": [
        "def AP(query_relevancy_labels):\n",
        "    # =======Your code=======\n",
        "    sum_of_relevancy_labels_times_precision_at_k = 0\n",
        "    \n",
        "    sum_of_relevancy_labels = np.sum(query_relevancy_labels)    \n",
        "    if sum_of_relevancy_labels == 0:\n",
        "        return 0\n",
        "    \n",
        "    relevancy_times_precision_per_k = [query_relevancy_labels[k] * precision(query_relevancy_labels, k + 1) for k in range(query_relevancy_labels.size)]\n",
        "    sum_of_relevancy_labels_times_precision_at_k = np.sum(np.asarray(relevancy_times_precision_per_k))\n",
        "    \n",
        "    return sum_of_relevancy_labels_times_precision_at_k / sum_of_relevancy_labels\n",
        "    # ======================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "EfC4brEYwiuE",
      "metadata": {
        "id": "EfC4brEYwiuE"
      },
      "outputs": [],
      "source": [
        "# Test your code\n",
        "assert np.isclose(AP(np.array([0,0,0,0,0], dtype=int)), 0)\n",
        "assert np.isclose(AP(np.array([0,0,0,0,1], dtype=int)), 0.2)\n",
        "assert np.isclose(AP(np.array([0,1,1,1,1], dtype=int)), 0.6791666666666667)\n",
        "assert np.isclose(AP(np.array([1], dtype=int)), 1)\n",
        "assert np.isclose(AP(np.array([0, 1], dtype=int)), 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72164664",
      "metadata": {
        "id": "72164664"
      },
      "source": [
        "### 5.7 Reciprocal Rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f55726b4",
      "metadata": {
        "id": "f55726b4"
      },
      "outputs": [],
      "source": [
        "def RR(query_relevancy_labels):\n",
        "    # =======Your code=======\n",
        "    query_relevancy_labels = query_relevancy_labels.tolist()\n",
        "    \n",
        "    if 1 not in query_relevancy_labels:\n",
        "        return 0\n",
        "            \n",
        "    return 1 / (query_relevancy_labels.index(1) + 1)\n",
        "    # ======================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "96wLvcVryeeg",
      "metadata": {
        "id": "96wLvcVryeeg"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(RR(np.array([0,0,0,0,0], dtype=int)), 0)\n",
        "assert np.isclose(RR(np.array([0,0,0,0,1], dtype=int)), 0.2)\n",
        "assert np.isclose(RR(np.array([0,1,1,1,1], dtype=int)), 0.5)\n",
        "assert np.isclose(RR(np.array([1], dtype=int)), 1)\n",
        "assert np.isclose(RR(np.array([0, 0, 1], dtype=int)), 0.3333333333333333)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31172639",
      "metadata": {
        "id": "31172639"
      },
      "source": [
        "## 6. Processing an Entire Results File\n",
        "\n",
        "Now that you have implementations for each of the metrics, we can compute their mean values averaged over all the queries in dataset.\n",
        "\n",
        "### 6.1 Computing Metric Means over an Entire Results File\n",
        "\n",
        "The following code will go over all queries in the files, compute the metric values and average them, it makes use of your implementation of the *process_files* function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7a1d5a5a",
      "metadata": {
        "id": "7a1d5a5a"
      },
      "outputs": [],
      "source": [
        "def evaluate(qrel_path, results_path):\n",
        "    results_per_query = {\n",
        "        'precision@1': [],\n",
        "        'precision@5': [],\n",
        "        'precision@10': [],\n",
        "        'precision@25': [],\n",
        "        'recall@1': [],\n",
        "        'recall@5': [],\n",
        "        'recall@10': [],\n",
        "        'recall@25': [],\n",
        "        'F-score@1': [],\n",
        "        'F-score@5': [],\n",
        "        'F-score@10': [],\n",
        "        'F-score@25': [],\n",
        "        'DCG@1': [],\n",
        "        'DCG@5': [],\n",
        "        'DCG@10': [],\n",
        "        'DCG@25': [],\n",
        "        'NDCG@1': [],\n",
        "        'NDCG@5': [],\n",
        "        'NDCG@10': [],\n",
        "        'NDCG@25': [],\n",
        "        'MAP': [],\n",
        "        'MRR': [],\n",
        "    }\n",
        "    for labels in process_files(qrel_path, results_path):\n",
        "        results_per_query['precision@1'].append(precision(labels, 1))\n",
        "        results_per_query['precision@5'].append(precision(labels, 5))\n",
        "        results_per_query['precision@10'].append(precision(labels, 10))\n",
        "        results_per_query['precision@25'].append(precision(labels, 25))\n",
        "        results_per_query['recall@1'].append(recall(labels, 1))\n",
        "        results_per_query['recall@5'].append(recall(labels, 5))\n",
        "        results_per_query['recall@10'].append(recall(labels, 10))\n",
        "        results_per_query['recall@25'].append(recall(labels, 25))\n",
        "        results_per_query['F-score@1'].append(F_score(labels, 1))\n",
        "        results_per_query['F-score@5'].append(F_score(labels, 5))\n",
        "        results_per_query['F-score@10'].append(F_score(labels, 10))\n",
        "        results_per_query['F-score@25'].append(F_score(labels, 25))\n",
        "        results_per_query['DCG@1'].append(DCG(labels, 1))\n",
        "        results_per_query['DCG@5'].append(DCG(labels, 5))\n",
        "        results_per_query['DCG@10'].append(DCG(labels, 10))\n",
        "        results_per_query['DCG@25'].append(DCG(labels, 25))\n",
        "        results_per_query['NDCG@1'].append(NDCG(labels, 1))\n",
        "        results_per_query['NDCG@5'].append(NDCG(labels, 5))\n",
        "        results_per_query['NDCG@10'].append(NDCG(labels, 10))\n",
        "        results_per_query['NDCG@25'].append(NDCG(labels, 25))\n",
        "        results_per_query['MAP'].append(AP(labels))\n",
        "        results_per_query['MRR'].append(RR(labels))\n",
        "    \n",
        "    results = {}\n",
        "    for key, values in results_per_query.items():\n",
        "        results[key] = np.mean(values)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4e2062",
      "metadata": {
        "id": "8b4e2062"
      },
      "source": [
        "### 6.2 Checking Part of Your Answer\n",
        "To make the process easier for you, we have provided you with the correct metrics for the first results file.\n",
        "\n",
        "Remember that when grading we will look at the computed metrics for **all three** result files: therefore passing this check does not guarantee that the assignment was done correctly.\n",
        "However, if this check is not passed you can be certain that your answer needs improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3ed5c495",
      "metadata": {
        "id": "3ed5c495",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CORRECTNESS METRIC-NAME    CORRECT  YOURS   ERROR\n",
            "--------------------------------------------------\n",
            "ACCEPTABLE  precision@1    0.45133 0.45133 0.00000\n",
            "ACCEPTABLE  precision@5    0.42290 0.42290 0.00000\n",
            "ACCEPTABLE  precision@10   0.38329 0.38329 0.00000\n",
            "ACCEPTABLE  precision@25   0.27206 0.27206 0.00000\n",
            "ACCEPTABLE  recall@1       0.07473 0.07473 0.00000\n",
            "ACCEPTABLE  recall@5       0.31573 0.31573 0.00000\n",
            "ACCEPTABLE  recall@10      0.51692 0.51692 0.00000\n",
            "ACCEPTABLE  recall@25      0.74827 0.74827 0.00000\n",
            "ACCEPTABLE  F-score@1      0.11454 0.11454 0.00000\n",
            "ACCEPTABLE  F-score@5      0.30638 0.30638 0.00000\n",
            "ACCEPTABLE  F-score@10     0.37767 0.37767 -0.00000\n",
            "ACCEPTABLE  F-score@25     0.35248 0.35248 0.00000\n",
            "ACCEPTABLE  DCG@1          0.45133 0.45133 0.00000\n",
            "ACCEPTABLE  DCG@5          1.26566 1.26566 0.00000\n",
            "ACCEPTABLE  DCG@10         1.81735 1.81735 0.00000\n",
            "ACCEPTABLE  DCG@25         2.53794 2.53794 0.00000\n",
            "ACCEPTABLE  NDCG@1         0.45133 0.45133 0.00000\n",
            "ACCEPTABLE  NDCG@5         0.47204 0.47204 0.00000\n",
            "ACCEPTABLE  NDCG@10        0.52043 0.52043 0.00000\n",
            "ACCEPTABLE  NDCG@25        0.59589 0.59589 0.00000\n",
            "ACCEPTABLE  MAP            0.49699 0.49699 0.00000\n",
            "ACCEPTABLE  MRR            0.58498 0.58498 0.00000\n"
          ]
        }
      ],
      "source": [
        "correct_answers = {\n",
        "    'precision@1': 0.4513313110886417,\n",
        "    'precision@5': 0.4228985507246377,\n",
        "    'precision@10': 0.3832895180316819,\n",
        "    'precision@25': 0.2720552746882373,\n",
        "    'recall@1': 0.07472577850117126,\n",
        "    'recall@5': 0.315725702455326,\n",
        "    'recall@10': 0.5169225080207952,\n",
        "    'recall@25': 0.7482686903944814,\n",
        "    'F-score@1': 0.11454143750486454,\n",
        "    'F-score@5': 0.30638131785149253,\n",
        "    'F-score@10': 0.37767235358763745,\n",
        "    'F-score@25': 0.35247610330560825,\n",
        "    'DCG@1': 0.4513313110886417,\n",
        "    'DCG@5': 1.2656562772756248,\n",
        "    'DCG@10': 1.8173462254841932,\n",
        "    'DCG@25': 2.5379411534956744,\n",
        "    'NDCG@1': 0.4513313110886417,\n",
        "    'NDCG@5': 0.4720439629436193,\n",
        "    'NDCG@10': 0.520432656270525,\n",
        "    'NDCG@25': 0.5958868199120942, \n",
        "    'MAP': 0.4969908565132953,\n",
        "    'MRR': 0.5849835056944714\n",
        "    }\n",
        "your_answers = evaluate('Data/qrel.txt', 'Data/results_1.txt')\n",
        "print('CORRECTNESS METRIC-NAME    CORRECT  YOURS   ERROR')\n",
        "print('--------------------------------------------------')\n",
        "for metric_name in correct_answers:\n",
        "    correct_a = correct_answers[metric_name]\n",
        "    your_a =  your_answers[metric_name]\n",
        "    error = (correct_a - your_a)\n",
        "    acceptable = np.abs(error) < 0.0001\n",
        "    output_tuple = (metric_name.ljust(14), correct_a, your_a, error)\n",
        "    if acceptable:\n",
        "        print('ACCEPTABLE  %s %0.05f %0.05f %0.05f' % output_tuple)\n",
        "    else:\n",
        "        print('INCORRECT   %s %0.5f %0.05f %0.05f' % output_tuple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3a53f8",
      "metadata": {
        "id": "2c3a53f8"
      },
      "source": [
        "## 7. Processing All Result Files and Handing In\n",
        "\n",
        "### 7.1 Evaluating and Outputting\n",
        "\n",
        "The following code will generate the files for you to hand in, update the *student* variables and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "12e915f3",
      "metadata": {
        "id": "12e915f3"
      },
      "outputs": [],
      "source": [
        "student_number = 'S1080742'\n",
        "student_first_name = 'DAAN'\n",
        "student_last_name = 'BRUGMANS'\n",
        "\n",
        "result = {\n",
        "    'STUDENTNUMBER': student_number,\n",
        "    'FIRSTNAME': student_first_name,\n",
        "    'LASTNAME': student_last_name,\n",
        "    'results_1': evaluate('Data/qrel.txt', 'Data/results_1.txt'),\n",
        "    'results_2': evaluate('Data/qrel.txt', 'Data/results_2.txt'),\n",
        "    'results_3': evaluate('Data/qrel.txt', 'Data/results_3.txt'),\n",
        "}\n",
        "\n",
        "output_path = 'Data/%s_%s_%s_evaluation_assignment.json' % (student_number, student_first_name, student_last_name)\n",
        "with open(output_path, 'w') as f:\n",
        "      json.dump(result, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11dde7a6",
      "metadata": {
        "id": "11dde7a6"
      },
      "source": [
        "### 7.2 Reflection\n",
        "\n",
        "Take a look at the metrics for the three results files in the json file you have just created, what observations do you make? are there conclusions we can make from these observations?<br>\n",
        "*(max 300 words)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c154e3",
      "metadata": {
        "id": "39c154e3"
      },
      "source": [
        "First, the precision@k for $k \\in \\{1, 5\\}$ is clearly higher for results_2 and results_3 than results_1. This would imply that the top 5 retrieved documents for queries in results_1 are generally less relevant than for the queries in results_2 and results_3, since the amount of retrieved, irrelevant documents is higher in results_1. However, as $k$ increases, the precision@k converges for all result files. The recall@k for $k \\in \\{5, 10, 25\\}$ is quite similar for all result files; only $k = 1$ shows a clearly worse performance for results_1. The F1-score@k seems to increase as k increases, but dips down at $k = 25$. This holds for all result files. I suspect that this has to do with the fact that in the F1-score@k, the sum of the precision@k and recall@k at the denominator grows faster at larger values of k than the product at the numerator. I suspect that originates from the high(ly increased) values of recall@25. Finally, NDCG@k values are very similar between results_2 and results_3, being highest at $k = 25$ where $NDCG@25 \\approx 0.69$. However, for results_1, $NDCG@25 \\approx 0.60$. In fact, for most values of $k$, the NDCG of results_1 is about .1 lower than the NDCG of results_2 and results_3. These metrics would lead me to conclude that the relevancy evaluations of the results_1 query-document pairs are more poor than those of results_2 and results_3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45606a86",
      "metadata": {
        "id": "45606a86"
      },
      "source": [
        "### 7.3 Handing in\n",
        "\n",
        "Hand in the json output file, the filled in notebook, and the converted notebook to **PDF format**.  Rename your files to:\n",
        "> STUDENTNUMBER_FIRSTNAME_LASTNAME_evaluation.ipynb\n",
        "> STUDENTNUMBER_FIRSTNAME_LASTNAME_evaluation.pdf\n",
        "> STUDENTNUMBER_FIRSTNAME_LASTNAME_evaluation_assignment.json"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
