{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZay1ryJyqEq"
   },
   "source": [
    "\n",
    "# TxMM Assignment 3: Information Extraction\n",
    "## Learning goals of this assignment:\n",
    "1. Become familiar with the intricacies and challenges of time expression labelling\n",
    "2. Practice with manual annotation using BIO tags\n",
    "3. Learn how to compute inter-annotator agreement scores with Cohen’s kappa, and understand how to interpret these kappa scores\n",
    "4. Define a set of patterns for extracting time patterns from text\n",
    "5. Calculate precision for the output of your code on an unseen text\n",
    "6. Gain insight into the importance of pattern generalizability\n",
    "7. Reflect on the gap between labeling time expressions and the actual IE task of extracting and matching events from texts.\n",
    "\n",
    "## Group Assignment\n",
    "\n",
    "This assignment is a group assignment. For this assignment, ensure you are enrolled in an \"Assignment 3 - Information Extraction\" group on Brightspace with a group of three people.\n",
    "\n",
    "\n",
    "## Practical information\n",
    "\n",
    "Note that the assignment will be graded with a Pass/Fail system.\n",
    "\n",
    "Whenever you have any questions, feel free to ask us in the open lunch hours on Mondays! You can also contact the TAs, Ellen Jansen and Heleen Visserman, through discord or by sending a mail to Heleen (heleen.visserman@ru.nl).\n",
    "\n",
    "We would appreciate it if you do not contact us via WhatsApp for non-urgent matters, so we can keep our TA work and private life (somewhat) separate.\n",
    "\n",
    "We only support the use of [Google Colab](https://https://colab.research.google.com/) as all assignments have been implemented and tested using this. In case of (strange) bugs on other platforms, please consider switching to Colab to make sure that we can provide with all the help you may need.\n",
    "\n",
    "## Handing in the assignment:\n",
    "Please hand in the following files:\n",
    "- This notebook containing your answers in a .ipynb format.\n",
    "- One .csv file containing all three of your BIO-tag annotations.\n",
    "\n",
    "Please run the notebook before you hand in the assignment.\n",
    "\n",
    "Only one of the group members has to hand in the assignment files on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQGEceLJyhAu"
   },
   "source": [
    "# The Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiECyZEdyj47"
   },
   "source": [
    "This assignment consists of two parts: the first part focuses on textual annotation of time expressions and the second part focuses on automatizing time expression extraction with regular expressions and normalizing them to a unified format.\n",
    "\n",
    "The goal of this assignment is to let you practice in annotation labeling and reflect on the challenges when doing manual annotation of time expressions in text.\n",
    "\n",
    "Recognizing time expressions is a crucial part of many NLP and IE applications. In this assignment we focus on date expressions in biography texts from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpFkvbbNBAJd"
   },
   "source": [
    "### Installation\n",
    "First, make sure pandas, plotly and nltk are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2eqen9DBAJd"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHhy9vvxi2oN"
   },
   "source": [
    "# Part 1 - Manual Annotation & Inter-Annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDx_fGCe7Nie"
   },
   "source": [
    "### Uploading files to Colab\n",
    "\n",
    "Similar to a previous assignment you need to upload some files that are needed for the assignment in the working directory. For this assignment you need to upload the files *Timnit_Gebru_bio_trainset.txt*,  *Geoffrey_Hinton_bio_devset.txt*,\n",
    "*Yoshua_Bengio_bio_testset.txt* and *utils.py*.\n",
    "\n",
    "Instead of using the upload functionality, you can also download the file directly in the notebook. Therefore you need to upload the files to https://transfer.sh/ then run the following command in a code cell using the URLs you created:\n",
    "\n",
    "```\n",
    "!wget <transfer.sh url>/Timnit_Gebru_bio_trainset.txt\n",
    "!wget <transfer.sh url>/Geoffrey_Hinton_bio_devset.txt\n",
    "!wget <transfer.sh url>/Yoshua_Bengio_bio_testset.txt\n",
    "!wget <transfer.sh url>/utils.py\n",
    "```\n",
    "\n",
    "Pay attention: The links expire after two weeks and you have to create new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS8uTLoqyNcM"
   },
   "source": [
    "Note that it is not at all important that you achieve and report a high Cohen's kappa for this task. In a real text mining application, researchers often go through multiple cycles of annotation rounds to come up with consistent and clear annotation guidelines. Here you are starting with an initial round of annotations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEuXpT8NyzhT"
   },
   "source": [
    "### Task 1: Perform Manual Annotation\n",
    "Each of you individually annotates the date expressions in the biography text of Timnit Gebru taken from Wikipedia without discussing the task together. This results in 3 versions (A,B,C) of the text with annotations.\n",
    "\n",
    "A **date expression** is a sequence within the text (can contain letters, numbers, and/or punctuation) that expresses a point in time or a period of time.\n",
    "\n",
    "As annotation labels we use the B (begin), I (inside) and O (outside) labels to indicate if a token is part of a time expression or not.\n",
    "\n",
    "We provide a text with one token per line as a starting point, you can find `Timnit_Gebru_bio_TPL.csv` in the provided zip file. We advise you to use Excel or another editor that allows you to save the resulting annotations as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IRg5glU7z5I"
   },
   "source": [
    "*After you individually created your annotated versions, combine them into one annotation file. Hand in the file together with this notebook on Brightspace.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF_EuHexzmKx"
   },
   "source": [
    "### Task 2: Compute Inter-Annotator Agreement\n",
    "Upload the csv file and compute Cohen’s kappa between each pair of annotations (AB,BC,AC) and report the 3 scores. Use sklearn.metrics.cohen_kappa_score for this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZy1w274zsK4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "### BEGIN SOLUTION\n",
    "import pandas as pd\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df = pd.read_csv(\"TxMM_A3-IE_31_s1101750_s1080742_annotations.csv\", sep=\"\\t\")\n",
    "print(cohen_kappa_score(df.janneke, df.daan))\n",
    "print(cohen_kappa_score(df.janneke, df.mats))\n",
    "print(cohen_kappa_score(df.mats, df.daan))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myepLAaA0RbB"
   },
   "source": [
    "1.0 (Janneke & Daan)\n",
    "0.9590737657571948 (Janneke & Mats)\n",
    "0.9590737657571948 (Mats & Daan)\n",
    "\n",
    "Because our group consists of two people, we asked Mats Robben (s1054883) for his annotation as well. He is also in a group of two people, so we provided them Janneke's annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Wi_Ndp8RjN"
   },
   "source": [
    "### Task 3: Reflect on the Cohen's Kappa score:\n",
    "#### 3.1) How do you interpret the kappa scores? Are all 3 scores similar?  What does a high score indicate?\n",
    "\n",
    "#### 3.2) If you place the three columns of BIO tags next to each other, what are the cases where you disagreed? Can you explain why?\n",
    "\n",
    "#### 3.3) Was the Kappa score metric suitable to evaluate the inter-annotator agreement on this task of time expression labeling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9sQRKxd8tTt"
   },
   "source": [
    "3.1) How do you interpret the kappa scores? Are all 3 scores similar?  What does a high score indicate? \\\n",
    "What we conclude from the Cohen's kappa values is that Janneke and Daan happened to annotate the exact same date expressions. Because these are the same, the value for both annotation when compared to Mats' annotation is the same. Mats has a slightly different annotation, but the value is still close to 1, so it is very similar.\n",
    "A high score (close to 1) indicates that the two sets of annotations are very similar.\n",
    "\n",
    "3.2) If you place the three columns of BIO tags next to each other, what are the cases where you disagreed? Can you explain why? \\\n",
    "summer\tB\tB\tO\n",
    "of  \tI\tI\tO\n",
    "2017\tI\tI\tB\n",
    "Mats apparently did not agree that \"summer\" is a time expression. Janneke and Daan do not agree, because although summer is not very specific, it definitely descibes a certain part of 2017.\n",
    "This is the only case we disagreed on.\n",
    "\n",
    "3.3) Was the Kappa score metric suitable to evaluate the inter-annotator agreement on this task of time expression labeling? \\\n",
    "Here, the annotation of the time expressions are quite rare compared to the other words in the text. This presence of correct annotation of the other words is not very important. It might be better to pay more attention to the B and I labels, or the scores at the positions in/around the time expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsKaC33yi_OA"
   },
   "source": [
    "# Part 2 - Regular Expressions and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT9zXBdrBAJf"
   },
   "source": [
    "## Extracting timelines and matching events from biographies\n",
    "### Loading & inspecting the Geoffrey_Hinton_bio_devset.txt\n",
    "\n",
    "In this second part you implement a IE program that automatically extracts time expresssions from texts. We focus here on events described in two texts (the biographies of Geoffrey Hinton and Yoshua Bengio) and you will find the matching events (i.e. overlapping dates) of the two timelines.\n",
    "\n",
    "Here, your goal is to identify and extract date expressions from sentences.\n",
    "\n",
    "First, you want to get an impression of the date expressions in the biography of Geoffrey Hinton.\n",
    "Run the next cell to see Geoffrey_Hinton_bio_devset.txt. *Don't look at Yoshua_Bengio_bio_testset.txt, an extraction of the biography of Yoshua Bengio yet!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzcCXt38BAJf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Make sure Geoffrey_Hinton_bio_devset.txt and Yoshua_Bengio_bio_testset.txt are in the working directory.\n",
    "\n",
    "def read_file(file_name):\n",
    "  with open(file_name, \"r\") as f:\n",
    "    return f.read()\n",
    "\n",
    "working_dir = os.getcwd()  # get our working directory\n",
    "train_file_path = os.path.join(working_dir, 'Geoffrey_Hinton_bio_devset.txt')\n",
    "test_file_path = os.path.join(working_dir, 'Yoshua_Bengio_bio_testset.txt')\n",
    "\n",
    "text_geoffrey = read_file(train_file_path)\n",
    "print(text_geoffrey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGciYNYkBAJg"
   },
   "source": [
    "#### Task 1: Examine the biography of Geoffrey Hinton, and notice the patterns in which the date expressions occur. List and describe your observations in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CMr_JYmBAJh"
   },
   "source": [
    "Dates are described like : \n",
    "- Day (as number) month year (as number, 4 numbers long)\n",
    "- Only a year (as number, 4 numbers long)\n",
    "- Time frame (year as number - year as number)\n",
    "- Month and year (year as number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap186JxsBAJh"
   },
   "source": [
    "#### Task 2: Implement the function *sentence_tokenize_text* so that it splits a text into a list of sentences.\n",
    "\n",
    "*Note: we split the assignment in two parts as the first part used a word-per-line format, while in this second part we look at sentence-per-line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jomV13DvBAJh"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import re\n",
    "### END SOLUTION\n",
    "\n",
    "def sentence_tokenize_text(text):\n",
    "    \"\"\"\n",
    "    :param text: An input text, i.e. a string\n",
    "    :return: A list of strings, where each string is one sentence\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return re.findall(r'[^\\n ][^.?!]*[.?!]', text)\n",
    "    ### END SOLUTION\n",
    "\n",
    "# sentence_tokenize_text(text_geoffrey)  # You can display the result for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w4p7pIcBAJh"
   },
   "source": [
    "#### Task 3: Implement the function *extract_date_expressions* so that it extracts date expressions from sentences.\n",
    "**To ensure unbiased annotation of the test data in task 9, only one person of your group will annotate the test data. This person cannot look at the regex that is being developed in this task. The other two can develop the regex together.**\n",
    "\n",
    "The function should take in our list of sentences and return a pandas DataFrame. This DataFrame has two columns:\n",
    "- Date: The extracted date expressions\n",
    "- Sentence: The sentence from which a data expression was extracted\n",
    "\n",
    "**Write your own patterns** and do not rely on libraries that automatically extract date expressions as learning about regular expressions is one of the learning objectives of the exercise.\n",
    "\n",
    "Use the Geoffrey_Hinton_dev.txt and the manual annotation that you did on the biography of Timnit Gebru as inspiration for your regular expressions.\n",
    "\n",
    "*Hint: Check out https://regexr.com/ for testing and refining the regular expressions you use to capture date expressions. It also has a handy cheat sheet you can use. *\n",
    "\n",
    "*For coding newbies: You can contact the TAs to get a regex example function in Python.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N8NDSJPBAJi"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import pandas as pd\n",
    "### END SOLUTION\n",
    "\n",
    "def extract_date_expressions(sentences):\n",
    "    \"\"\"\n",
    "    :param sentences: A list of strings, where each string is one sentence\n",
    "    :return: A pandas DataFrame with the columns\n",
    "                \"Date\" (extracted date expressions as a string)\n",
    "                \"Sentence\" (sentences from which a date expression was extracted)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    solution = pd.DataFrame(columns = [\"Dates\", \"Sentence\"])\n",
    "    for sentence in sentences:\n",
    "        matches = re.findall(r'(?:(?:(?P<day>[0-9]{1,2})? (?P<month>January|February|March|April|May|June|July|August|September|October|November|December))? (?P<year>[0-9]{4}))', sentence)\n",
    "        for match in matches:\n",
    "            solution.loc[len(solution)] = [\" \".join(match).strip(), sentence]\n",
    "    return solution\n",
    "\n",
    "# Apply the function to the tokenized text:\n",
    "df_dates_geoffrey =  extract_date_expressions(sentence_tokenize_text(text_geoffrey))\n",
    "df_dates_geoffrey # use this for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQAoFxzQBAJi"
   },
   "source": [
    "#### Task 4: Normalization: Implement the function *dates_to_iso8601* so that it converts a date expression string to the ISO 8601 date standard.\n",
    "Then, add the ISO 8601 converted dates as a column (\"ISO\") to our dataframe.\n",
    "\n",
    "You can find more info and examples on https://www.iso.org/iso-8601-date-and-time-format.html.\n",
    "\n",
    "To implement the function check out Python’s inbuilt datetime module. you’ll find functions in there that can make this task a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJ6vsatpBAJi"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def date_expression_to_iso8601(date_string):\n",
    "    \"\"\"\n",
    "    :param date_string: A string containing a date expression\n",
    "    :return: A string containing the date in ISO 8601 format\n",
    "    \"\"\"\n",
    "    match = re.search(r'^(?P<day>[0-9]{1,2})?[ ]?(?P<month>[A-z]+)?[ ]?(?P<year>[0-9]{4})?$', date_string)\n",
    "    day, month, year = match.group(\"day\"), match.group(\"month\"), match.group(\"year\")\n",
    "    day = \"0\"*(2-len(day)) + day if day else \"\"\n",
    "    month = month if month else \"\"\n",
    "    year = year if year else \"\"\n",
    "    \n",
    "    if month != \"\":\n",
    "        datetime_object = datetime.strptime(month[:3], \"%b\")\n",
    "        month = str(datetime_object.month) if datetime_object.month >= 10 else f\"0{datetime_object.month}\"\n",
    "    try:\n",
    "        return pd.to_datetime(f\"{year}-{month}-{day}\".strip(\"-\"))\n",
    "    except:\n",
    "        print(f\"{year}-{month}-{day}\".strip(\"-\"))\n",
    "        return \"AAA\"\n",
    "\n",
    "# Now, add a column \"ISO\" to your DataFrame\n",
    "### BEGIN SOLUTION\n",
    "df_dates_geoffrey.index = df_dates_geoffrey[\"Dates\"].map(date_expression_to_iso8601)\n",
    "df_dates_geoffrey.index.name = \"ISO\"\n",
    "df_dates_geoffrey = df_dates_geoffrey.sort_index()\n",
    "### END SOLUTION\n",
    "df_dates_geoffrey  # use this for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msKupXKgFjmH"
   },
   "source": [
    "#### Task 5: In which respect does the ISO 8601 format defer from the date information present in the text? (If you noticed an additional issue caused by a discrepancy between the standard and the Python implementation please mention this as well.)\n",
    "\n",
    "Janneke: The ISO format requires a day, month and year, however, these are often not all present in the text. This makes it harder to translate all of the date information to ISO format. Also, including just a month in a date format is not allowed, so I had to ignore these, while I originally added them to the regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Jr-ekx2BAJj"
   },
   "source": [
    "#### Task 6: Combine the previous steps in the function *get_sorted_df_from_file_name* so that it runs the whole date extraction pipeline and returns a DataFrame.\n",
    "**Make sure to order the DataFrame rows chronologically according to the ISO dates!**\n",
    "\n",
    "Consider the following example text:\n",
    "\n",
    "> This is an example text about interesting upcoming dates. Halloween takes place on 31 October 2021. Our Christmas holiday is from Friday 24 December 2021 - Friday 7 January 2022. We will celebrate Sinterklaas on 5 December 2021.\n",
    "\n",
    "Here's an illustration of what the example text's DataFrame should look like:\n",
    "\n",
    "|ISO |Date | Sentence |\n",
    "|----:|----:|:----|\n",
    "|2021-10-31 |31 October 2021| Halloween takes place on 31 October 2021.|\n",
    "|2021-12-05 |5 December 2021| We will celebrate Sinterklaas on 5 December 2021.|\n",
    "|2021-12-24 |24 December 2021| Our Christmas holiday is from Friday 24 December 2021 - Friday 7 January 2022.|\n",
    "|2022-01-07 |7 January 2022| Our Christmas holiday is from Friday 24 December 2021 - Friday 7 January 2022.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LknbnUtxBAJj"
   },
   "outputs": [],
   "source": [
    "def get_sorted_df_from_file_name(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: A string containing the full path to a file\n",
    "    :return: A pandas DataFrame with the columns \"Date\", \"Sentence\" and \"ISO\"\n",
    "          (see above), where rows are sorted according to \"ISO\"\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Extract dates, then sort by ISO\n",
    "    text = read_file(file_name)\n",
    "    tokenized_text = sentence_tokenize_text(text)\n",
    "    df_dates =  extract_date_expressions(tokenized_text)\n",
    "    df_dates.index = df_dates[\"Dates\"].map(date_expression_to_iso8601)\n",
    "    df_dates.index.name = \"ISO\"\n",
    "    df_dates = df_dates.sort_index()\n",
    "    return df_dates\n",
    "    ### END SOLUTION\n",
    "\n",
    "get_sorted_df_from_file_name(train_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNno2UGrBAJj"
   },
   "source": [
    "### Manual labeling: Geoffrey_Hinton_bio_devset.txt\n",
    "To evaluate your date expression pipeline, you first need to have gold labels. For Part 2 we use another labeling format to simplify the regex matching. Let's familiarize ourselves with this format.\n",
    "\n",
    "Look at our example text again:\n",
    "\n",
    "> This is an example text about interesting upcoming dates. Halloween takes place on 31 October 2021. Our Christmas holiday is from Friday 24 December 2021 - Friday 7 January 2022. We will celebrate Sinterklaas on 5 December 2021.\n",
    "\n",
    "\n",
    "This is how we store the gold labels for this example text:\n",
    "~~~python\n",
    "example_manual_labels = [\n",
    "  {\"Dates\": [],\n",
    "    \"Sentence\": \"This is an example text about interesting upcoming dates.\"},\n",
    "  {\"Dates\": [\"2021-10-31\"],\n",
    "    \"Sentence\": \"The next Halloween takes place on 31 October 2021.\"},\n",
    "  {\"Dates\": [\"2021-12-24\", \"2022-01-07\"],\n",
    "    \"Sentence\": \"Our Christmas holiday is from Friday 24 December 2021-Friday 7 January 2022.\"},\n",
    "  {\"Dates\": [\"2021-12-05\"],\n",
    "    \"Sentence\": \"We will celebrate Sinterklaas on 5 December 2021.\"}\n",
    "  ]\n",
    "~~~\n",
    "\n",
    "As you can see, we use a list of dictionaries. We have one dictionary for each sentence. This dictionary has two keys:\n",
    "- Key \"Sentence\": The corresponding value is the sentence (i.e. a string).\n",
    "- Key \"Dates\": The corresponding value is a list of all date expressions (strings; correctly converted to the ISO 8601 date standard) that appear in that sentence. If a sentence does not contain any date expressions, this list is empty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BphCKIsc4TBS"
   },
   "source": [
    "We've created a helper function that automatially provides a template list based on *your sentence_tokenize_text* function. Run the cell below to get this template for *text_geoffrey*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9obC2LWc4NRG"
   },
   "outputs": [],
   "source": [
    "def get_manual_labeling_list(text):\n",
    "  \"\"\"\n",
    "  A helper function to print a manual labeling list in which you only have to\n",
    "  fill in the dates for each sentence.\n",
    "  :param text: An input text, i.e. a string\n",
    "  \"\"\"\n",
    "  tokenized_text = sentence_tokenize_text(text)\n",
    "  return [{\"Dates\": [], \"Sentence\": tokenized_text[i]} for i in range(len(tokenized_text))]\n",
    "\n",
    "get_manual_labeling_list(text_geoffrey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L3mGXA94l9C"
   },
   "source": [
    "#### Task 7: Manually label all sentences from Geoffrey_Hinton_bio_devset.txt.   \n",
    "Copy the template list output by our helper function for Geoffrey_Hinton_bio_devset.txt into the cell below. Then manually fill the list *geoffrey_manual_labels* with the dates (in correct ISO 8601 format) from each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjSQaAyoBAJj"
   },
   "outputs": [],
   "source": [
    "geoffrey_manual_labels = [\n",
    "  {'Dates': [\"1947-12-06\"],\n",
    "  'Sentence': 'Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks.'},\n",
    " {'Dates': [\"2013\", \"2023\", \"2023-05\"],\n",
    "  'Sentence': 'From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing his departure from Google in May 2023 citing concerns about the risks of artificial intelligence (AI) technology.'},\n",
    " {'Dates': [\"2017\"],\n",
    "  'Sentence': 'In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.'},\n",
    " {'Dates': [], \n",
    "  'Sentence': 'With David Rumelhart and Ronald J.'},\n",
    " {'Dates': [\"1986\"],\n",
    "  'Sentence': 'Williams, Hinton was co-author of a highly cited paper published in 1986 that popularised the backpropagation algorithm for training multi-layer neural networks, although they were not the first to propose the approach.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'Hinton is viewed as a leading figure in the deep learning community.'},\n",
    " {'Dates': [\"2012\"],\n",
    "  'Sentence': 'The dramatic image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky and Ilya Sutskever for the ImageNet challenge 2012 was a breakthrough in the field of computer vision.'},\n",
    " {'Dates': [\"2018\"],\n",
    "  'Sentence': 'Hinton received the 2018 Turing Award (often referred to as the \"Nobel Prize of Computing\"), together with Yoshua Bengio and Yann LeCun, for their work on deep learning.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'They are sometimes referred to as the \"Godfathers of Deep Learning\", and have continued to give public talks together.'},\n",
    " {'Dates': [\"2023-05\"],\n",
    "  'Sentence': 'In May 2023, Hinton announced his resignation from Google to be able to \"freely speak out about the risks of A.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'He has voiced concerns about deliberate misuse by malicious actors, technological unemployment, and existential risk from artificial general intelligence.'},\n",
    " {'Dates': [], \n",
    "  'Sentence': \"Hinton was educated at King's College, Cambridge.\"},\n",
    " {'Dates': [\"1970\"],\n",
    "  'Sentence': 'After repeatedly changing his degree between different subjects like natural sciences, history of art, and philosophy, he eventually graduated in 1970 with a bachelor of arts in experimental psychology.'},\n",
    " {'Dates': [\"1978\"],\n",
    "  'Sentence': 'He continued his study at the University of Edinburgh where he was awarded a PhD in artificial intelligence in 1978 for research supervised by Christopher Longuet-Higgins.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': \"Hinton's research concerns ways of using neural networks for machine learning, memory, perception, and symbol processing.\"},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'He has written or co-written more than 200 peer reviewed publications.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'Hinton co-invented Boltzmann machines with David Ackley and Terry Sejnowski.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'His other contributions to neural network research include distributed representations, time delay neural network, mixtures of experts, Helmholtz machines and Product of Experts.'},\n",
    " {'Dates': [\"2007\"],\n",
    "  'Sentence': 'In 2007, Hinton coauthored an unsupervised learning paper titled Unsupervised learning of image transformations.'},\n",
    " {'Dates': [\"1992-09\", \"1993-10\"],\n",
    "  'Sentence': \"An accessible introduction to Geoffrey Hinton's research can be found in his articles in Scientific American in September 1992 and October 1993.\"},\n",
    " {'Dates': [], \n",
    "  'Sentence': 'Text from https://en.'},\n",
    " {'Dates': [], \n",
    "  'Sentence': 'wikipedia.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'org/wiki/Geoffrey_Hinton, adapted by the teaching team.'}]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSS73IigBAJk"
   },
   "source": [
    "Now that we have labels for Geoffrey_Hinton_bio_devset.txt, we can plot a confusion matrix to get an impression of your extraction procedure's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFu-hHSm1Eor"
   },
   "outputs": [],
   "source": [
    "from utils import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(manual_labels = geoffrey_manual_labels,\n",
    "                      sorted_date_df = get_sorted_df_from_file_name(train_file_path),\n",
    "                      normalize    = False,\n",
    "                      title_names = ['Positive','Negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BccP6ZMlBAJk"
   },
   "source": [
    "#### Task 8.1: Manually calculate the precision and recall of your date expression procedure on Geoffrey_Hinton_bio_devset.txt.\n",
    "Write your calculation into the cell below. Write your *full* calculation including the formula you are using.\n",
    "\n",
    "Hint: You can use $\\LaTeX$ here.\n",
    "- To display a formula inline, surround it by the '\\$' sign.\n",
    "    - For example, '\\$ 4=2^2 \\$' will be displayed like this: $4=2^2$\n",
    "- To display a formula in a block, surround it by '\\$\\$'.\n",
    "    - For example, '\\$\\$ 16=4^2 \\$\\$' will be displayed like this: $$16=4^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qKbw4XwBAJk"
   },
   "source": [
    "Precision = $\\frac{TP}{TP+FP}$ \\\n",
    "Recall = $\\frac{TP}{TP+FN}$ \\\n",
    "\\\n",
    "Precision = $\\frac{13}{13+0} = \\frac{13}{13} = 1$ \\\n",
    "Recall = $\\frac{13}{13+0} = \\frac{13}{13} = 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCYrbW-xBAJk"
   },
   "source": [
    "**Now, repeat the following steps until you are satisfied with the performance:**\n",
    "1. Run the date expression procedure on Geoffrey_Hinton_bio_devset.txt.\n",
    "2. Make adaptations to your code if necessary.\n",
    "3. Go through the output manually and calculate precision and recall. **Make sure the cell above contains the latest calculation.**\n",
    "\n",
    "Once you are satisfied with your performance, proceed to the next part of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOg3z_DR-7m2"
   },
   "source": [
    "#### Task 8.2: Discuss the difficulties you encountered during each repeat of the above steps to develop the time patterns.  ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzuItyK2Idzw"
   },
   "source": [
    "Splitting sentences is quite hard, as \".\" can exist in a sentence without this marking the end of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMs1nDlJBAJk"
   },
   "source": [
    "### Applying the extraction procedure to the unseen Yoshua_Bengio_bio_testset.txt\n",
    "Next, we will test your date extraction procedure and see how it performs on the unseen file Yoshua_Bengio_bio_testset.txt. First, let's have a look at the text inside this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sadqgWXRBAJk"
   },
   "outputs": [],
   "source": [
    "text_yoshua = read_file(test_file_path)\n",
    "print(text_yoshua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2nQw4xmBAJl"
   },
   "source": [
    "#### Task 9: Manually label all sentences from Yoshua_Bengio_bio_testset.txt\n",
    "**The one person that didn't work on the regex has to be the person to annotate the test file**\n",
    "\n",
    "The following cell gives you the template list.\n",
    "Fill the list *Yoshua_manual_labels* (just like you previously did for Geoffrey_Hinton_bio_devset.txt) in the cell under the following one with the dates from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be0nekKH5jdo"
   },
   "outputs": [],
   "source": [
    "get_manual_labeling_list(text_yoshua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPHu0xu2BAJn"
   },
   "outputs": [],
   "source": [
    "yoshua_manual_labels = [\n",
    "{'Dates': [\"1964-03-05\"],\n",
    "  'Sentence': 'Yoshua Bengio OC FRS FRSC (born March 5, 1964]) is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'He is a professor at the Department of Computer Science and Operations Research at the UniversitÃ© de MontrÃ©al and scientific director of the Montreal Institute for Learning Algorithms (MILA).'},\n",
    " {'Dates': [\"2018\"], \n",
    "  'Sentence': 'Bengio received the 2018 ACM A.'},\n",
    " {'Dates': [], \n",
    "  'Sentence': 'M.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'Turing Award (often referred to as the \"Nobel Prize of Computing\"), together with Geoffrey Hinton and Yann LeCun, for their work on deep learning.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'Bengio, Hinton, and LeCun, are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\".'},\n",
    " {'Dates': [\"2023-05\"],\n",
    "  'Sentence': 'As of May 2023, he is the most cited computer scientist by h-index.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'Bengio was born in France to a Jewish family who immigrated to France from Morocco, and then immigrated again to Canada.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'He received his Bachelor of Science degree (electrical engineering), MSc (computer science) and PhD (computer science) from McGill University.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': \"The Bengio brothers lived in Morocco for a year during their father's military service there.\"},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'His father, Carlo Bengio, was a pharmacist who wrote theatre pieces and ran a Sephardic theatrical troupe in Montreal that played Judeo-Arabic pieces.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'His mother, CÃ©lia Moreno, is also an artist who played in one of the major theatre scenes of Morocco that was run by Tayeb Seddiki in the 1970s.'},\n",
    " {'Dates': [], \n",
    "  'Sentence': 'Career and research.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'After his PhD, Bengio was a postdoctoral fellow at MIT (supervised by Michael I.'},\n",
    " {'Dates': [], \n",
    "  'Sentence': 'Jordan) and AT&T Bell Labs.'},\n",
    " {'Dates': [\"1993\"],\n",
    "  'Sentence': 'Bengio has been a faculty member at the Université de Montréal since 1993 and is co-director of the Learning in Machines & Brains program at the Canadian Institute for Advanced Research.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'Along with Geoffrey Hinton and Yann LeCun, Bengio is considered by Cade Metz as one of the three people most responsible for the advancement of deep learning during the 1990s and 2000s.'},\n",
    " {'Dates': [\"2018\"],\n",
    "  'Sentence': 'Among the computer scientists with an h-index of at least 100, Bengio was as of 2018 the one with the most recent citations per day, according to MILA.'},\n",
    " {'Dates': [\"2022-12\"],\n",
    "  'Sentence': 'As of December 2022, he had the 2nd highest Discipline H-index (D-index) in computer science.'},\n",
    " {'Dates': [\"2019\"],\n",
    "  'Sentence': 'Thanks to a 2019 article on a novel RNN architecture, Bengio has an Erdős number of 3.'},\n",
    " {'Dates': [], 'Sentence': \n",
    "   'Views on AI.'},\n",
    " {'Dates': [\"2023-05\"],\n",
    "  'Sentence': 'Following concerns raised by AI experts about the existential risks AI poses on humanity, in May 2023, Bengio stated in an interview to BBC that he felt \"lost\" over his life\\'s work.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'He raised his concern about \"bad actors\" getting hold of AI, especially as it becomes more sophisticated and powerful.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'He called for better regulation, product registration, ethical training, and more involvement from governments in tracking and auditing AI products.'},\n",
    " {'Dates': [\"2023-05\"],\n",
    "  'Sentence': 'Speaking with the Financial Times also in May 2023, Bengio said that he supported the monitoring of access to AI systems such as ChatGPT so that potentially illegal or dangerous uses could be tracked.'},\n",
    " {'Dates': [], \n",
    "  'Sentence': 'Awards and honours.'},\n",
    " {'Dates': [\"2017\"],\n",
    "  'Sentence': 'In 2017, Bengio was named an Officer of the Order of Canada.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'The same year, he was nominated Fellow of the Royal Society of Canada and received the Marie-Victorin Quebec Prize.'},\n",
    " {'Dates': [\"2018\"],\n",
    "  'Sentence': 'Together with Geoffrey Hinton and Yann LeCun, Bengio won the 2018 Turing Award.'},\n",
    " {'Dates': [\"2020\"],\n",
    "  'Sentence': 'In 2020 he was elected a Fellow of the Royal Society.'},\n",
    " {'Dates': [\"2022\"],\n",
    "  'Sentence': 'In 2022 he received the Princess of Asturias Award in the category \"Scientific Research\" with his peers Yann LeCun, Geoffrey Hinton and Demis Hassabis.'},\n",
    " {'Dates': [\"2023\"],\n",
    "  'Sentence': \"In 2023 Bengio was appointed Knight of the Legion of Honour, France's highest order of merit.\"},\n",
    " {'Dates': [], 'Sentence': 'Text from https://en.'},\n",
    " {'Dates': [], 'Sentence': 'wikipedia.'},\n",
    " {'Dates': [],\n",
    "  'Sentence': 'org/wiki/Yoshua_Bengio, adapted by the teaching team.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LOC_Y0e5AOn"
   },
   "source": [
    "**Now, let's run your date expression procedure on the unseen text *Yoshua_Bengio_bio_testset.txt* and look at the resulting DataFrame.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6Q37zlx5jdQ"
   },
   "outputs": [],
   "source": [
    "get_sorted_df_from_file_name(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ByoHYvMBAJn"
   },
   "source": [
    "#### Task 10:  Make adaptations to your date extraction code if necessary. Do not change the functions from the previous part in this assignment, but make your adjustments by changing the three functions below.\n",
    "\n",
    "Currently, each of these \"adapted\" functions just uses the functions from the previous parts. If you want to make any changes to one of the functions, overwrite this return statement with your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjDEzbp2BAJo"
   },
   "outputs": [],
   "source": [
    "def sentence_tokenize_text_adapted(text):\n",
    "  \"\"\"\n",
    "  :param text: An input text, i.e. a string\n",
    "  :return: A list of strings, where each string is one sentence\n",
    "  \"\"\"\n",
    "  # change this if you want to adapt your original function\n",
    "  return sentence_tokenize_text(text)\n",
    "\n",
    "\n",
    "def extract_date_expressions_adapted(sentences):\n",
    "  \"\"\"\n",
    "  :param sentences: A list of strings, where each string is one sentence\n",
    "  :return: A pandas DataFrame with the columns\n",
    "                \"Date\" (extracted date expressions as a string)\n",
    "                \"Sentence\" (sentences from which a date expression was extracted)\n",
    "  \"\"\"\n",
    "  # change this if you want to adapt your original function\n",
    "  return extract_date_expressions(sentences)\n",
    "\n",
    "\n",
    "def date_expression_to_iso8601_adapted(date_string):\n",
    "  \"\"\"\n",
    "  :param date_string: A string containing a date expression\n",
    "  :return: A string containing the date in ISO 8601 format\n",
    "  \"\"\"\n",
    "  # change this if you want to adapt your original function\n",
    "  return date_expression_to_iso8601(date_string)\n",
    "\n",
    "\n",
    "def get_sorted_df_from_file_name_adapted(file_name):\n",
    "  \"\"\"\n",
    "  :param file_name: A string containing the full path to a file\n",
    "  :return: A pandas DataFrame with the columns \"Date\", \"Sentence\" and \"ISO\"\n",
    "          (see above), where rows are sorted according to \"ISO\"\n",
    "  \"\"\"\n",
    "  # change this if you want to adapt your original function\n",
    "  return get_sorted_df_from_file_name(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBlpaFHOBAJo"
   },
   "source": [
    "#### Task 11: Discuss the difficulties you encountered extracting the new timeline. Also address the adaptations that you needed to make for processing the unseen biography Yoshua_Bengio_bio_testset.txt.\n",
    "\n",
    "Write your discussion into the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYiKrWtjBAJo"
   },
   "source": [
    "There were no difficulties. All functions ran without errors. Therefore, we did not need to make any adaptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOJbQGVj5hwV"
   },
   "source": [
    "**Now, we can evaluate your adapted date expression procedure. Let's plot one confusion matrix for each of the text files. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZhQEZpi4-fE"
   },
   "outputs": [],
   "source": [
    "print('Confusion matrix for Geoffrey Hinton:')\n",
    "plot_confusion_matrix(manual_labels = geoffrey_manual_labels,\n",
    "                      sorted_date_df = get_sorted_df_from_file_name_adapted(train_file_path),\n",
    "                      normalize    = False,\n",
    "                      title_names = ['Positive','Negative'])\n",
    "\n",
    "print('Confusion matrix for Yoshua Bengio:')\n",
    "plot_confusion_matrix(manual_labels = yoshua_manual_labels,\n",
    "                      sorted_date_df = get_sorted_df_from_file_name_adapted(test_file_path),\n",
    "                      normalize    = False,\n",
    "                      title_names = ['Positive','Negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mis3cKpXBAJo"
   },
   "source": [
    "#### Task 12: Calculate the precision and recall of your adapted date expression procedure on both Geoffrey_Hinton_bio_devset.txt and Yoshua_Bengio_bio_testset.txt.\n",
    "\n",
    "Write your calculations into the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXBmTI_jBAJo"
   },
   "source": [
    "Precision = $\\frac{TP}{TP+FP}$ \\\n",
    "Recall = $\\frac{TP}{TP+FN}$ \\\n",
    "\\\n",
    "Precision = $\\frac{13}{13+1} = \\frac{13}{12} = 0.929 $ \\\n",
    "Recall = $\\frac{13}{13+1} = \\frac{13}{12} = 0.929 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuyKThl2BAJp"
   },
   "source": [
    "#### Task 13: Compare the precision and recall scores on Geoffrey_Hinton_bio_devset.txt and Yoshua_Bengio_bio_testset.txt. Address the following points:\n",
    "- Did the values of Geoffrey_Hinton_bio_devset.txt change after your adaptions?\n",
    "- Are there any differences between the two texts?\n",
    "- If so, where does this difference come from?\n",
    "- What does this difference mean in terms of generalizability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwZM7OBsBAJp"
   },
   "source": [
    "There were no adaptions.\n",
    "Yes, there are differences. In the second text, there is a date that has the day, month and year in a different order, making it hard to extract this date.\n",
    "It makes it hard to extract every possible date sequence based on a pattern, because dates can be described in many ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_yDquKREY5C"
   },
   "source": [
    "#### Task 14: Take a closer look at the errors your automatic extraction method makes. What is happening there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0-Enf23IVMS"
   },
   "source": [
    "False Negative: {'Dates': ['1964-03-05'], 'Sentence': 'Yoshua Bengio OC FRS FRSC (born March 5, 1964]) is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning.'}\n",
    "\n",
    "Here, the date is in an order that the automatic extraction method did not expect. This causes the method not to recognise this date correctly.\n",
    "\n",
    "False Positive: {'Dates': [], 'Sentence': 'Along with Geoffrey Hinton and Yann LeCun, Bengio is considered by Cade Metz as one of the three people most responsible for the advancement of deep learning during the 1990s and 2000s.'}\n",
    "\n",
    "Here, the method recognized 1990 and 2000 as date expressions, but the manual labelling did not. This is because the manual labelling assumed that '1990s' and '2000s' did not refer to specific dates or periods, but to broad, general, prolonged periods of time, while the method recognized '1990' and '2000' as specific date expressions. This mismatch caused an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ISR7Wx7IY2w"
   },
   "source": [
    "#### Task 15: Reflect on the relation between date mentions in the texts and the events that they denote, reflect on duration and overlap of events. (max. 4 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wXoPYvWIevI"
   },
   "source": [
    "Some date expressions that only consist of a mentioned year are not necessarily an expression of a certain moment; they are part of the name of an event that occured, like the \"2018 ACM A.M. Turing Award\". Such a title implies that the event took place in 2018, but without further context isn't a definite confirmation, as it might have taken place at the end of 2017 or the beginning of 2019. Another type of date mentions in relation to events are (very) broad periods of time, like \"[...] Bengio is considered [...] one of the three people most responsible for the advancement of deep learning during the 1990s and 2000s.\" Here, there is a mention of a period from the 1990s and 2000s, but this date expression is so broad and non-specific to a certain event, that it may be uncertain whether this is considered a date expression at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQEmxaF3BAJp"
   },
   "source": [
    "### Finding matching events\n",
    "#### Task 16: Find the matching events (i.e. overlapping dates in the two biographical timelines) between the two timelines.\n",
    "\n",
    "You could do this in one of the following ways:\n",
    "- **Command line** We highly encourage you to use the ‘comm’ function on your ISO-8601 dates in your command line (more info and example: https://www.computerhope.com/unix/ucomm.htm. Note that this command also works on Mac OS/Windows).\n",
    "- **Python** It is also allowed to find the matching events programmatically in Python.\n",
    "- **Online tool ** You can use an online tool (e.g. https://text-compare.com/).\n",
    "\n",
    "**Important: Do not search for matching events manually!**\n",
    "\n",
    "Please put the command, the code or the link to the website you used into the cell below or briefly describe how you found the matching events.  \n",
    "\n",
    "Then, also paste the list of matching events (in ISO-8601 format) into that cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seCjCDB5BAJp"
   },
   "source": [
    "The code is shown in the codeblock below\n",
    "\n",
    "List: \\\n",
    "1970-01-01 \\\n",
    "2017-01-01 \\\n",
    "2018-01-01 \\\n",
    "2023-01-01 \\\n",
    "2023-05-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_sorted_df_from_file_name_adapted(train_file_path)\n",
    "test_df = get_sorted_df_from_file_name_adapted(test_file_path)\n",
    "list(train_df.index.intersection(test_df.index, sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMoICcPCBAJq"
   },
   "source": [
    "#### Task 17: Discussion of matching events\n",
    "1. Discuss the list of matching events that you found.\n",
    "2. When going through the texts manually, do you find matching events that you did not find programmatically/automatically?\n",
    "3. If so, what could be the reason(s) for this? Discuss your answers to these questions and any other difficulties you encountered during the extraction of matching events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9EtialZcvRS"
   },
   "source": [
    "1. We mostly found years, not full dates with month and days. This makes sense, because these are less unique. Mind that an expressin like 1970-01-01 means that we found only 1970. The pandas timestamp object adds the month and the day to this expression.\n",
    "2. Manually, we found that the year 2018 in our list does not only occur in both texts, but it both relates to the 2018 Turing award.\n",
    "3. The name of the award/event is required to recognise that this is more than an overlapping date. However, the expresions we extract only extracts the date and not the name of the event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuRKcGAw-5wG"
   },
   "outputs": [],
   "source": [
    "print('Congratulations! You finished this assignment!')\n",
    "print('We would like to thank Theo Kent for letting us use his notebook as a basis for this assignment!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKT37cC8x7KZ"
   },
   "source": [
    "Please look at the ***Handing in the assignment*** section for instructions on what to hand in."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
