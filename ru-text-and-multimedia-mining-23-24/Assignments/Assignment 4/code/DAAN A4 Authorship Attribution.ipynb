{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TxMM Assignment 4 - Authorship Attribution\n",
    "\n",
    "This "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Daan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Daan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Daan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It\"s got a lot of action, a lot of heart, come...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiro: (blushes in response) AniUniverse: (chuc...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Girl with the Baymax onesie: (stands up) Thank...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am too! Anyways, this is where we let the fa...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Do you like sushi? Do you find your name ironi...</td>\n",
       "      <td>1276465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "1  It\"s got a lot of action, a lot of heart, come...  1276465\n",
       "2  Hiro: (blushes in response) AniUniverse: (chuc...  1276465\n",
       "3  Girl with the Baymax onesie: (stands up) Thank...  1276465\n",
       "4  I am too! Anyways, this is where we let the fa...  1276465\n",
       "5  Do you like sushi? Do you find your name ironi...  1276465"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.tokenize.destructive\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"tagsets\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"Data\")\n",
    "\n",
    "def load_fanfiction_data(data_purpose: str) -> pd.DataFrame:\n",
    "    path_to_data_csv = os.path.join(data_dir, f\"pan2324_{data_purpose}_data.csv\")\n",
    "    \n",
    "    return pd.read_csv(path_to_data_csv, index_col=0).sort_index()\n",
    "\n",
    "train_data = load_fanfiction_data(\"train\")\n",
    "test_data = load_fanfiction_data(\"test\")\n",
    "dev_data = load_fanfiction_data(\"dev\")\n",
    "\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing\n",
    "\n",
    "In order to extract features from the text data, it might be required that some preprocessing steps are taken first. To this end, a collection of text preprocessing functions are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_punctuation(word_tokenized_text: list[str]) -> list[str]:\n",
    "    word_tokenized_text_no_punctuation = []\n",
    "    \n",
    "    for word in word_tokenized_text:\n",
    "        if word in list(string.punctuation):\n",
    "            continue\n",
    "        \n",
    "        word_tokenized_text_no_punctuation.append(word)\n",
    "        \n",
    "    return word_tokenized_text_no_punctuation\n",
    "\n",
    "def _remove_digits(word_tokenized_text: list[str]) -> list[str]:\n",
    "    word_tokenized_text_no_digits = []\n",
    "    \n",
    "    for word in word_tokenized_text:\n",
    "        if word in list(string.digits):\n",
    "            continue\n",
    "        \n",
    "        word_tokenized_text_no_digits.append(word)\n",
    "        \n",
    "    return word_tokenized_text_no_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature extraction\n",
    "\n",
    "For this assignment, a set of different features are used. These features may be split up into a number of feature categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = \"I'm looking for a research project where I can study the language Esperanto from a linguistic perspective. That subject's called Esperantology. I think that's quite nifty :). 10/10 idea, great, greater, greatest!. Gabbagool!!!\"\n",
    "val_text_word_tokenized = nltk.word_tokenize(val_text)\n",
    "val_text_sent_tokenized = nltk.sent_tokenize(val_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Count features\n",
    "\n",
    "Count features are lexical features that the describe the count of something within the text, like the count of words or sentences in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of all characters of the given parameter string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of characters of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all characters in the `text` parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(text)\n",
    "\n",
    "def word_count(word_tokenized_text: str) -> int:\n",
    "    \"\"\"Returns the count of all words that occur in the input string. \n",
    "    This count is determined using NLTK's `word_tokenize` function.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string to compute the amount of words of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all words in the `word_tokenized_text` parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(word_tokenized_text)\n",
    "\n",
    "def sentence_count(sentence_tokenized_text: str) -> int:\n",
    "    \"\"\"Returns the count of all sentences that occur in the input string. \n",
    "    This count is determined using NLTK's `sent_tokenize` function.\n",
    "    \n",
    "    Args:\n",
    "        - `sentence_tokenized_text`: The string to compute the amount of sentences of.\n",
    "        \n",
    "    Returns:\n",
    "        The count of all sentences in the `sentence_tokenized_text` parameter.\n",
    "    \"\"\"\n",
    "    return len(sentence_tokenized_text)\n",
    "\n",
    "def punctuation_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of punctuation occurrences in the input string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of punctuation occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a form of punctuation occurs in the `text` parameter.    \n",
    "    \"\"\"\n",
    "    \n",
    "    punctuation_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.punctuation):\n",
    "            punctuation_count += 1\n",
    "    \n",
    "    return punctuation_count\n",
    "\n",
    "def digit_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of individual digits occurring in the input string.\n",
    "    Note that this does not mean numbers, e.g., \"23\" will return \"2\", since the number 23 consists of two digits.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of digit occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a digit occurs in the `text` parameter.\n",
    "    \"\"\"\n",
    "    digit_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.digits):\n",
    "            digit_count += 1\n",
    "            \n",
    "    return digit_count\n",
    "\n",
    "def uppercase_count(text: str) -> int:\n",
    "    \"\"\"Returns the count of uppercase characters occurring in the input string.\n",
    "    Note that only uppercase characters that are part of ASCII are supported.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string to compute the amount of uppercase character occurrences of.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times an uppercase character occurs in the `text` parameter.\n",
    "    \"\"\"\n",
    "    uppercase_count = 0\n",
    "    \n",
    "    for character in text:\n",
    "        if character in list(string.ascii_uppercase):\n",
    "            uppercase_count += 1\n",
    "            \n",
    "    return uppercase_count\n",
    "\n",
    "def short_word_count(word_tokenized_text: List[str], short_word_max_length: int = 4) -> int:\n",
    "    \"\"\"Returns the count of \"short\" words that occur in the `word_tokenized_text` parameter.\n",
    "    The cutoff point for \"short\" words is given by the `short_word_max_length` parameter.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string to compute the amount of \"short\" words of.\n",
    "        - `short_word_max_length`: The maximum length of what is considered to be a \"short\" word. This length is inclusive.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a \"short\" word occurs in the `word_tokenized_text` parameter.\n",
    "    \"\"\"     \n",
    "    short_word_list = [word for word in word_tokenized_text if len(word) <= short_word_max_length]\n",
    "    \n",
    "    return len(short_word_list)\n",
    "\n",
    "def alphabet_count(text: List[str], include_uppercase: bool = True, include_punctuation: bool = True, include_digits: bool = True) -> int:\n",
    "    \"\"\"Returns the length of the alphabet of the given text. A text's alphabet is defined as all unique characters that occur in that text.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to compute an alphabet for.\n",
    "        - `include_uppercase`: A boolean used to determine whether to count uppercase characters as separate from their lowercase counterparts.\n",
    "        - `include_punctuation`: A boolean used to determine whether to include punctuation in the alphabet.\n",
    "        - `include_digits`: A boolean used to determine whether to include digits in the alphabet.\n",
    "        \n",
    "    Returns:\n",
    "        The length of the alphabet of the `text` variable.\n",
    "    \"\"\"\n",
    "    if not include_uppercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    text_char_alphabet = set(text)\n",
    "    \n",
    "    if not include_punctuation:\n",
    "        text_char_alphabet = {char for char in text_char_alphabet if char not in list(string.punctuation)}\n",
    "        \n",
    "    if not include_digits:\n",
    "        text_char_alphabet = {char for char in text_char_alphabet if char not in list(string.digits)}\n",
    "    \n",
    "    return len(text_char_alphabet)\n",
    "\n",
    "def contraction_count(text: List[str], include_genetive_count: bool = True) -> int:\n",
    "    \"\"\"Returns the count of all contractions that occur in the given string.\n",
    "    \n",
    "    Args:\n",
    "        - `text`: The string for which to count the number of occurring contractions.\n",
    "        - `include_genetive_count`: A boolean used to determine if occurrences of the genetive should count towards the number of contractions found.\n",
    "        \n",
    "    Returns:\n",
    "        The amount of contractions that occur in the `text` variable.\n",
    "    \"\"\"\n",
    "    contraction_count = len(contractions.preview(text, 1))\n",
    "    \n",
    "    if include_genetive_count:\n",
    "        tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "        pos_tagged_text = nltk.tag.pos_tag(tokenized_text)\n",
    "        genetive_count = len([tag for _, tag in pos_tagged_text if tag == \"POS\"])\n",
    "        \n",
    "        contraction_count += genetive_count\n",
    "        \n",
    "    return contraction_count\n",
    "\n",
    "def word_without_vowels_count(word_tokenized_text: List[str], include_y_as_vowel: bool = False) -> int:\n",
    "    \"\"\"Returns the count of words in the input string that do not contain vowels.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string for which to count the number of words without vowels.\n",
    "        - `include_y_as_vowel`: A boolean used to determine if the character \"y\" should be counted as a vowel.\n",
    "        \n",
    "    Returns:\n",
    "        The number of times a word without vowels occurs in the input string.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_without_vowels_count = 0\n",
    "    vowels = set(\"aeiou\")\n",
    "    \n",
    "    if include_y_as_vowel:\n",
    "        vowels.add(\"y\")\n",
    "    \n",
    "    word_tokenized_text = _remove_punctuation(word_tokenized_text)\n",
    "    word_tokenized_text = _remove_digits(word_tokenized_text)\n",
    "    \n",
    "    for word in word_tokenized_text:        \n",
    "        if len(vowels.intersection(word)) == 0:\n",
    "            word_without_vowels_count += 1\n",
    "    \n",
    "    return word_without_vowels_count\n",
    "\n",
    "def hapax_legomenon_count(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"Returns the count of hapax legomenon in the input text.\n",
    "    A hapax legomenon is a word that occurs only once in a corpus.\n",
    "    Note that for the purposes of this function, the corpus is considered to be the input text.\n",
    "    \n",
    "    Args:\n",
    "        - `word_tokenized_text`: The string for which to count the number of hapax legomenon.\n",
    "        \n",
    "    Returns:\n",
    "        The number of hapax legomena that were found in the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    hapax_legomenon_count = 0\n",
    "    \n",
    "    word_tokenized_text = [word.lower() for word in word_tokenized_text]\n",
    "        \n",
    "    for word in word_tokenized_text:      \n",
    "        if word_tokenized_text.count(word) == 1:\n",
    "            hapax_legomenon_count += 1\n",
    "            \n",
    "    return hapax_legomenon_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text complexity features\n",
    "\n",
    "Text complexity features aim to describe the complexity of the given text. This may be at the lexical or the syntactical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_length(word_tokenized_text: List[str]) -> int:   \n",
    "    word_lengths = [len(word) for word in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.mean(word_lengths), 3)\n",
    "\n",
    "def mean_sentence_length(word_tokenized_text: List[str]) -> int:   \n",
    "    sentence_lengths = [len(sentence) for sentence in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.mean(sentence_lengths), 3)\n",
    "\n",
    "def word_length_standard_deviation(word_tokenized_text: List[str]) -> int:    \n",
    "    word_lengths = [len(word) for word in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.stdev(word_lengths), 3)\n",
    "\n",
    "def sentence_length_standard_deviation(word_tokenized_text: List[str]) -> int:    \n",
    "    sentence_lengths = [len(sentence) for sentence in word_tokenized_text]\n",
    "    \n",
    "    return round(statistics.stdev(sentence_lengths), 3)\n",
    "\n",
    "def mean_word_frequency(word_tokenized_text: List[str]) -> int:    \n",
    "    word_frequencies = {}\n",
    "    \n",
    "    for word in word_tokenized_text:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "                        \n",
    "    return round(statistics.mean(word_frequencies.values()), 3)\n",
    "\n",
    "def lexical_diversity_coefficient(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"From http://repository.utm.md/handle/5014/20225\"\"\"\n",
    "    total_word_count = word_count(word_tokenized_text)\n",
    "    unique_word_count = hapax_legomenon_count(word_tokenized_text)\n",
    "    \n",
    "    lexical_diversity_coefficient = unique_word_count / total_word_count\n",
    "    \n",
    "    return round(lexical_diversity_coefficient, 3)\n",
    "\n",
    "def syntactic_complexity_coefficient(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"From http://repository.utm.md/handle/5014/20225\"\"\"\n",
    "    total_word_count = word_count(word_tokenized_text)\n",
    "    total_sentence_count = sentence_count(word_tokenized_text)\n",
    "    \n",
    "    syntactic_complexity_coefficient = 1 - total_sentence_count / total_word_count\n",
    "    \n",
    "    return round(syntactic_complexity_coefficient, 3)\n",
    "\n",
    "def herdans_log_type_token_richness(word_tokenized_text: List[str]) -> int:\n",
    "    \"\"\"From https://pubs.asha.org/doi/abs/10.1044/jshr.3203.536\"\"\"\n",
    "    total_word_count = word_count(word_tokenized_text)\n",
    "    unique_word_count = hapax_legomenon_count(word_tokenized_text)\n",
    "    \n",
    "    herdans_log_type_token_richness = math.log(unique_word_count) / math.log(total_word_count)\n",
    "    \n",
    "    return round(herdans_log_type_token_richness, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Part-of-speech features\n",
    "\n",
    "These features describe the ratios of words that belong to a specific part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_ratios(text_word_tokenized: List[str], pos_tag_groups: Dict[str, List[str]]) -> Dict[str, int]:\n",
    "    total_word_count = len(text_word_tokenized)\n",
    "    pos_tag_ratios = {}\n",
    "    \n",
    "    text_pos_tags = [tag for _, tag in nltk.pos_tag(text_word_tokenized)]\n",
    "    \n",
    "    for key, value in pos_tag_groups.items():\n",
    "        pos_tag_ratios[key] = 0\n",
    "        \n",
    "        for pos_tag in value:\n",
    "            pos_tag_ratios[key] += text_pos_tags.count(pos_tag)\n",
    "            \n",
    "        pos_tag_ratios[key] = round(pos_tag_ratios[key] / total_word_count, 3)\n",
    "    \n",
    "    return pos_tag_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analytics base table\n",
    "\n",
    "The analytics base table, ABT for short, is the table that contains all the extracted features for all instances of the dataset. The analytics base table is fed to a machine learning algorithm in order to train a model. Here, the ABT for the fanfiction dataset is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abt(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    features = []\n",
    "    \n",
    "    dataset[\"text_word_tokenized\"] = dataset[\"text\"].apply(nltk.word_tokenize)\n",
    "    dataset[\"text_sent_tokenized\"] = dataset[\"text\"].apply(nltk.sent_tokenize)\n",
    "    \n",
    "    pos_tag_groups = {\n",
    "        \"Common Noun\": [\"NN\", \"NNS\"],\n",
    "        \"Proper Noun\": [\"NNP\", \"NNPS\"],\n",
    "        \"Base Adjective\": [\"JJ\",],\n",
    "        \"Comparative Adjective\": [\"JJR\",],\n",
    "        \"Superlative Adjective\": [\"JJS\",],\n",
    "        \"Base Adverb\":  [\"RB\",],\n",
    "        \"Comparative Adverb\": [\"RBR\",],\n",
    "        \"Superlative Adverb\": [\"RBS\",],\n",
    "        \"Infinitive Verb\": [\"VB\",],\n",
    "        \"Present Tense 1st/2nd Person Verb\": [\"VBP\",],\n",
    "        \"Present Tense 3rd Person Verb\": [\"VBZ\",],\n",
    "        \"Past Tense Verb\": [\"VBD\",],\n",
    "        \"Present Participle Verb\": [\"VBG\",],\n",
    "        \"Past Participle Verb\": [\"VBN\",],\n",
    "        \"Modal Auxiliary Verb\": [\"MD\",],\n",
    "        \"Pronoun\": [\"PRP\", \"PRP$\"],\n",
    "        \"Genetive\": [\"POS\",],\n",
    "        \"Interjection\": [\"UH\",],\n",
    "        \"Foreign Word\": [\"FW\",],\n",
    "        \"Numeral\": [\"CD\",],\n",
    "        \"Parenthesis\": [\"(\", \")\"]\n",
    "    }\n",
    "    \n",
    "    features.append(dataset[\"text\"].apply(character_count))\n",
    "    features.append(dataset[\"text\"].apply(word_count))\n",
    "    features.append(dataset[\"text\"].apply(punctuation_count))\n",
    "    features.append(dataset[\"text\"].apply(digit_count))\n",
    "    features.append(dataset[\"text\"].apply(uppercase_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(short_word_count))\n",
    "    features.append(dataset[\"text\"].apply(alphabet_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(word_without_vowels_count))\n",
    "    features.append(dataset[\"text\"].apply(contraction_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(hapax_legomenon_count))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(mean_word_length))\n",
    "    features.append(dataset[\"text_sent_tokenized\"].apply(mean_sentence_length))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(word_length_standard_deviation))\n",
    "    features.append(dataset[\"text_sent_tokenized\"].apply(sentence_length_standard_deviation))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(mean_word_frequency))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(lexical_diversity_coefficient))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(syntactic_complexity_coefficient))\n",
    "    features.append(dataset[\"text_word_tokenized\"].apply(herdans_log_type_token_richness))\n",
    "    \n",
    "    feature_df = pd.DataFrame(features).T\n",
    "    \n",
    "    abt = pd.concat(dataset.index, feature_df, dataset[\"author\"])\n",
    "    \n",
    "    return abt\n",
    "\n",
    "pos_tag_groups = {\n",
    "    \"Common Noun\": [\"NN\", \"NNS\"],\n",
    "    \"Proper Noun\": [\"NNP\", \"NNPS\"],\n",
    "    \"Base Adjective\": [\"JJ\",],\n",
    "    \"Comparative Adjective\": [\"JJR\",],\n",
    "    \"Superlative Adjective\": [\"JJS\",],\n",
    "    \"Base Adverb\":  [\"RB\",],\n",
    "    \"Comparative Adverb\": [\"RBR\",],\n",
    "    \"Superlative Adverb\": [\"RBS\",],\n",
    "    \"Infinitive Verb\": [\"VB\",],\n",
    "    \"Present Tense 1st/2nd Person Verb\": [\"VBP\",],\n",
    "    \"Present Tense 3rd Person Verb\": [\"VBZ\",],\n",
    "    \"Past Tense Verb\": [\"VBD\",],\n",
    "    \"Present Participle Verb\": [\"VBG\",],\n",
    "    \"Past Participle Verb\": [\"VBN\",],\n",
    "    \"Modal Auxiliary Verb\": [\"MD\",],\n",
    "    \"Pronoun\": [\"PRP\", \"PRP$\"],\n",
    "    \"Genetive\": [\"POS\",],\n",
    "    \"Interjection\": [\"UH\",],\n",
    "    \"Foreign Word\": [\"FW\",],\n",
    "    \"Numeral\": [\"CD\",],\n",
    "    \"Parenthesis\": [\"(\", \")\"]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
