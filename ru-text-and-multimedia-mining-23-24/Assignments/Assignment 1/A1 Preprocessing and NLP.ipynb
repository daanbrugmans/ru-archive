{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IWUfTDvUelh"
   },
   "source": [
    "# TxMM 2023/24\n",
    "## Assignment 1: Preprocessing & NLP\n",
    "\n",
    "Imagine you have a scientific interest in what people dream about. What are frequent motifs in dreams? Many of us have probably dreamed about falling from great heights or socially embarassing situations, but which is more common?\n",
    "\n",
    "To find out, we could recruit a number of volunteers and ask them to keep a dream journal. This would give us a small reliable sample of dream reports. As an alternative to gather much more data, we can try tapping a source of information that's close to lots of people, namely their Twitter feeds.\n",
    "\n",
    "As a researcher you must be aware that your data sample choice is not a representative reflection of actual dream behavior in general. The scope of your study is limited by several biases (in this case: limited to people who are on Twitter, only those dreams they are willing to share; they do not tweet exhaustively, but tweet when they have the time).\n",
    "\n",
    "In this study we try to answer the following research question:\n",
    "What are the dream themes most frequently described on Twitter?\n",
    "\n",
    "Learning goals of this assignment:\n",
    "\n",
    "- Get hands-on experience with text preprocessing and the characteristics of textual data\n",
    "- Learn that cleaning and filtering of textual data is not a simple or trivial task\n",
    "- Learn to convert a reseach question to a set of steps to find an answer to the question\n",
    "- Learn that looking at and into your data set in essential to grasp whether your data set is actually suited to answer your research question\n",
    "- Get acquainted with some tools for natural language processing (NLP)\n",
    "\n",
    "Note that the assignment will be graded with a Pass/Fail system.\n",
    "\n",
    "Whenever you have any questions, **make sure to checkout the FAQ at the bottom of this notebook**. If you cannot find your answer there or if you have some conceptual questions, you can simply talk on discord, join the office hours on monday, or send a mail to Heleen (heleen.visserman@ru.nl). We would appreciate it if you do not contact us via WhatsApp for non-urgent matters, so we can keep our TA work and private life (somewhat) separate.\n",
    "\n",
    "_We only support the use of [Google Colab](https://https://colab.research.google.com/) as all assignments have been implemented and tested using this. In case of (strange) bugs on other platforms, please consider switching to Colab to make sure that we can provide with all the help you may need._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcqcZgfWDsi"
   },
   "source": [
    "Most of you will have worked with Python notebooks before, but for some, this might be entirely new. Here is a brief introduction on how they work. You can skip this section if you feel confident in your knowledge of notebooks.\n",
    "\n",
    "## Brief notebook tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDaffl9-Gh4n"
   },
   "source": [
    "### Starting a notebook server\n",
    "\n",
    "The easiest way to run a notebook is to upload the notebook file to a service such as [Google Colab](https://colab.research.google.com) or [Kaggle](https://www.kaggle.com/kernels). These sites offer free computational resources that you can use to run notebooks and a relatively hassle-free, as you don't have to install Python, Jupyter, or any supporting libraries. We recommend the use of Google Colab, as this is the platform that all assignments have been implemented and tested on.\n",
    "\n",
    "If you would like to run the notebook on your own machine instead, you can follow the tutorial [here](https://jupyter.readthedocs.io/en/latest/install.html).\n",
    "\n",
    "### Running the notebook\n",
    "\n",
    "Once you have your notebook server running (either on a website or locally), you can start executing code. Notebooks are great in cases where you want to interleave code and text, e.g. when you want to perform a data analysis and provide some commentary of the findings along the side. They are not so great for production-level systems, e.g. when you work for a company and are tasked with designing a component to perform some textual analytics task. For these purposes, scripts should be used. In later assignments, we'll also use those, but for now, notebooks are ideal. With growing experience, you'll get a feeling for when each tool is appropriate.\n",
    "\n",
    "Notebooks distinguish between code cells and text cells. Code cells may contain arbritrary Python code. They can be executed in arbritary order. Text cells contain markup. When executed, they will change and show the formatted output of the markup. In this assignment, use code cells to perform text mining tasks and text cells to comment on findings or answer questions we ask you. To get you started, here's a simple question. Answer every question marked with a **Q** directly in the same cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5uPE7lX_Ele"
   },
   "source": [
    "**Q:** What cell types can you find in a notebook?\n",
    "\n",
    "**A:** Two types: code cells and text cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWKzKuVrAA-g"
   },
   "source": [
    "The following code cell contains a simple print statement. The output of all the statements will be shown below the cell that produced them, so the string will be printed below the following cell. Run the cell, then maybe change one or two of the variables to see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMoJv3An8d9K"
   },
   "outputs": [],
   "source": [
    "course_name = 'TxMM'\n",
    "course_year = 2023\n",
    "\n",
    "print(f'Welcome to {course_name}, {course_year} edition!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBQ7PALiBE-c"
   },
   "source": [
    "An important thing to keep in mind is that variables that you declare in one cell stay in memory once it has been executed. This is a powerful tool, as it allows you to refer back to variables you've already computed and to effectively split your task in managable chunks. Like every powerful tool, it also involves complexity: Make sure to execute cells in the correct order, because otherwise variables might not have their intended values when a cell refers to them. Even worse, they might not even be instantiated, giving rise to an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtOvSrfjBpNd"
   },
   "outputs": [],
   "source": [
    "print(f'Next year we\\'ll welcome students for {course_name}, {course_year + 1} edition.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzPpZ28IB-cc"
   },
   "source": [
    "Try changing one of the variables above without running the cell where they are declared. You'll notice that when you run the second code cell, the values won't get updated.\n",
    "\n",
    "### Importing and installing packages\n",
    "Notebooks rely on a *kernel* or *runtime* to execute Python code. This is a separate Python process. When a call to a function crashes, you might get a message about the kernel having died. Sometimes, you might also need to manually restart the kernel. As with every Python process, this one comes with a number of pre-installed packages, such as *os* for system calls to for example read files or *re* for regular expressions. Importing these packages is easy and works just like in normal scripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i45LT9EACwh8"
   },
   "outputs": [],
   "source": [
    "# import the system library\n",
    "import re\n",
    "\n",
    "input_string = 'TxMM is good fun.'\n",
    "# use a function from the library\n",
    "match = re.search('fun', input_string)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XDpt05HFF3r"
   },
   "source": [
    "In contrast, some packages might not be installed. In this assignment, we'll use some packages that you have to install yourself. In a normal Python environment, you can use *pip* to install packages on the command line. To do the same in a notebook, run the command in a code cell, but make sure to prepend it with an exclamation mark. This tells the kernel to hand this command to the system command line. See the following cell for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l81AN4uSFvvn"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk==3.4\n",
    "# I disabled the pip install cells because I do those manually.\n",
    "# Furthermore, this version of NLTK broke my Python version (3.11), \n",
    "# so I installed the latest NLTK version that is compatible with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhpkQvyCnNet"
   },
   "source": [
    "If the package was installed in the first place, the output will read *Requirement already satisfied*. If not, you should get a message informing about the successful installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pki5h-ZUHcUm"
   },
   "source": [
    "### Uploading (data) files to Colab\n",
    "In case you're using Google Colab, you also need to upload any other files you need, such as your data. On your left hand side, you can click the 'folder' icon. After the menu opens, you can click the upload button and select any file you need to upload. For this assignment, you'll need to upload the *tweets_clean_2021.txt* file from Brightspace in the *sample_data* folder. Note that every time you restart runtime, you need to upload the file(s) again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qVMnBjOmGhj"
   },
   "source": [
    "*Note about uploading the text file to Colab: Some people have experienced problems with uploading the file on Windows. Instead of using the upload functionality, you can also download the file directly in the notebook. Run the following command in a code cell to do so:*\n",
    "\n",
    "!wget http://transfer.sh/w7MrwM/tweets_clean_2021.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5d1PkrXFxvK"
   },
   "source": [
    "These are the most important things you need to know about notebooks. We'd recommend you to just get started now, but if you run into any trouble, you can check out the following [blogpost](https://www.dataquest.io/blog/jupyter-notebook-tutorial/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK6STTYHG0DR"
   },
   "source": [
    "## Loading data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqHcfA5zG5Zz"
   },
   "source": [
    "In this assignment, we'll look at collection of roughly 96,000 tweets crawled from the Twitter API between June and August 2014. Every tweet contains the phrase *I dreamed* or the phrase *I have dreamt*. Rather than storing each tweet in a separate file, all tweets have been dumped to a single file. Each line corresponds to a single tweet. For each tweet, the following information is stored:\n",
    "\n",
    "*   **phrase**: the query phrase that matched the tweet\n",
    "*   **tweet_id**: a unique tweet id\n",
    "*   **username**: the name of the user that sent the tweet\n",
    "*   **time**: the exact time the tweet was sent\n",
    "*   **tweet_text**: the actual text that was sent\n",
    "\n",
    "Individual chunks of information are separated by a tab character.\n",
    "\n",
    "*In case you're using Colab, and you have not yet uploaded the data file tweets_clean_2021.txt, please do so. For more explanation, see the brief notebook tutorial above.*\n",
    "\n",
    "First, we'll need to define a function that iterates over the whole file line by line, splits the line into chunks, and then returns a list of parsed tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZH6V_dYCrn3"
   },
   "source": [
    "Task 1: Implement the function below. It takes a path as its arguments and returns a list of lists. Each tweet is a list of the chunks we found in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jArD7d29ICiC"
   },
   "outputs": [],
   "source": [
    "# hint: although usually an library call is good as the first start, try to only use inbuilt functions for this\n",
    "def load_data(data_path):\n",
    "  tweets_file = open(data_path, \"r\", encoding=\"utf-8\")\n",
    "  tweets = []\n",
    "\n",
    "  for row in tweets_file.readlines():\n",
    "    data_point = row.split(\"\\t\")\n",
    "    tweets.append(data_point)\n",
    "\n",
    "  return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCVzCkEZMzhH"
   },
   "source": [
    "Let's load the data and briefly inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sXjHXSor-_6"
   },
   "outputs": [],
   "source": [
    "# Note: if you did not put the txt file in the sample_data folder, then you'll need to change the path below\n",
    "import os\n",
    "\n",
    "path_to_data = os.path.join(os.getcwd(), \"Data\", \"tweets_clean_2021.txt\")\n",
    "tweets = load_data(path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr21C4W1sC1I"
   },
   "source": [
    "The following cell contains some assert statements. They check if a condition hold and print a string if otherwise. If there's no output after running the cell, your function should have worked correctly. If not, use the string output to find out what is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcwFNWbuMmKq"
   },
   "outputs": [],
   "source": [
    "assert len(tweets) == 96118,'Not all tweets were loaded.'\n",
    "assert isinstance(tweets[0],list),'Every tweet should be a list of elements.'\n",
    "assert len(tweets[0]) == 5, 'Every tweet should contain exactly five elements.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4lBJEyjsAoC"
   },
   "outputs": [],
   "source": [
    "# print some tweets\n",
    "print(tweets[54])\n",
    "print(tweets[7])\n",
    "print(tweets[683])\n",
    "print(tweets[12612])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z8mMi33PHVI"
   },
   "source": [
    "Having the tweets as a list of lists is possible, but doesn't allow us to sort or filter them easily. We can use a package called pandas to make this task easier: Our collection of tweets becomes a *DataFrame* object, which is essentially just a big table. Every tweet is a row in the table, whereas each of the individual information chunks becomes a column. Later on, we'll see why this is useful. First, we'll install and import pandas and then load the tweets into a DataFrame object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "El5pkifgPwB8"
   },
   "outputs": [],
   "source": [
    "# Install and import pandas\n",
    "# !pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rItwHhXzUcs7"
   },
   "outputs": [],
   "source": [
    "# Define the column names\n",
    "names = ['phrase', 'tweet_id', 'username', 'time', 'tweet_text']\n",
    "# Create the dataframe with tweets as data and names as column names\n",
    "tweets = pd.DataFrame(tweets, columns = names)\n",
    "# Parse the time stamp strings to date objects we can sort on\n",
    "tweets['time'] =  pd.to_datetime(tweets['time'])\n",
    "# Sort on the date and reset the index\n",
    "tweets = tweets.sort_values('time')\n",
    "tweets = tweets.reset_index(drop=True)\n",
    "\n",
    "# Print the shape of the dataframe and the first few rows\n",
    "print(tweets.shape)\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwz4R2VvgRNw"
   },
   "source": [
    "## Text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqdZefuIWSRV"
   },
   "source": [
    "Let's first get an overview of the dataset. There's a lot of tweets in there, but we don't know how many of them contain genuine descriptions of dreams. To take a dive and inspect a number of tweets in an easy manner, an interactive scatter plot is the right tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLEbjIrajcVD"
   },
   "outputs": [],
   "source": [
    "# Compute a new column (or Series) by using the map function on an existing column\n",
    "# Each element in tweet_text gets fed to the len function\n",
    "# The results get stored in the new column\n",
    "tweets['length'] = tweets.tweet_text.map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSCC8NILsYy1"
   },
   "source": [
    "The following cell installs a specific version of the plotting library we want to use. If a previous version is already installed, it will get uninstalled automatically, but there might be output asking you to restart the runtime / kernel. Do so via the menu above and then run the cells above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz8yOQ-BEBuu"
   },
   "outputs": [],
   "source": [
    "# You might have to restart your runtime after this line executes\n",
    "# See FAQ in case this raises an error\n",
    "# !pip install plotly=='5.5.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUoQL-6EYD2G"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Plot some random 50 tweets, including their text in the hover field\n",
    "px.scatter(tweets[5000:5050],x='time',y='length',hover_data=['tweet_text'], color_discrete_sequence=px.colors.qualitative.Plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip44rFI3oQ38"
   },
   "source": [
    "Q1: Look at the 50 tweets in the plot, reading their content as you hover over them.\n",
    "\n",
    "1.1) Roughly, what percentage of those are genuine descriptions of what people dreamed of on the previous nights? \\\\\n",
    "1.2) When people are not tweeting about what they are dreaming about, what is their intention? Describe two different intentions that you notice.\n",
    "\n",
    "A: \n",
    "- 1.1 I'd say about 20% of the data are about what people have dreamt in a past night.\n",
    "- 1.2 Generally, if not tweeting about the actual dreams that they had, they are either talking about something that they aspire (\"[...] I dreamed all my life of [winning] that cup\"), or they are talking about the song \"I Dreamed A Dream\" from the musical Lés Miserables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yHPaaKJQd36"
   },
   "source": [
    "## Cleaning data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMak5HnBQip3"
   },
   "source": [
    "Unfortunately, the world is a messy place. Data is hardly ever ready for analysis or model-training, but instead requires manual cleaning. In this section of the assignment, we'll take some steps specifically needed for social media analysis.\n",
    "\n",
    "As we are interested in those tweets that contain actual dream descriptions, we aim to filter out duplicates and retweets as they are not expressing personal dream experiences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM7JHGRZUmmZ"
   },
   "source": [
    "### Finding retweets\n",
    "\n",
    "In the export format, retweets always contain 'RT', followed by a space, and a '@'.\n",
    "\n",
    "Task 2: Change the function stub below to capture retweets *as defined above*, so they can be removed. The function takes a string and should return a boolean that indicates whether the string is a retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VV0YGZ6QhPn"
   },
   "outputs": [],
   "source": [
    "# A function that indicates whether a string is a retweet\n",
    "def isRetweet(string): #write YOUR_CODE_HERE\n",
    "    return \"RT @\" in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdOSW1OpEcXW"
   },
   "outputs": [],
   "source": [
    "test_1 = isRetweet('RT @Tester this is a test!')\n",
    "assert test_1 == True, 'Your function missed a retweet!'\n",
    "test_2 = isRetweet('This is also a retweet RT @Tester This is a tweet')\n",
    "assert test_2 == True, 'Your function missed a retweet!'\n",
    "test_3 = isRetweet('@Someone: RT @Tester: Some tweet text.')\n",
    "assert test_3 == True, 'Your function missed a retweet!'\n",
    "test_4 = isRetweet('We render everything in RT, that is real-time!')\n",
    "assert test_4 == False, 'Your function spotted a RT in error.'\n",
    "test_5 = isRetweet('Talking about a RT doesn\\'t necessarily mean it is one')\n",
    "assert test_5 == False, 'Your function spotted a RT in error.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV03OI3DFGZi"
   },
   "source": [
    "Task 3: Now, change the code below to apply the function to the tweet_text and store the results in the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w257Y47KVf6V"
   },
   "outputs": [],
   "source": [
    "tweets['is_retweet'] = tweets['tweet_text'].map(isRetweet)\n",
    "\n",
    "n_retweets = sum(tweets.is_retweet)\n",
    "\n",
    "assert n_retweets == 20346, f'Your function stopped {n_retweets}, it should have spotted 20346.'\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soxHPdbbV9vn"
   },
   "source": [
    "### Finding duplicate tweets\n",
    "\n",
    "Certain users might tweet the same statement over and over. Let's remove these tweets too. We'll make a new column called 'is_duplicate' that contains a boolean that indicates whether a tweet is a duplicate.\n",
    "\n",
    "Task 4: Complete the code below, using the right function from the pandas API. Two tweets should be considered duplicates if their tweet_text fields are identical. All other fields should be disregarded. Pass the parameter to the function to not mark the first occurrence as a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hh51Bru_V1hG"
   },
   "outputs": [],
   "source": [
    "# drop duplicate tweets\n",
    "\n",
    "tweets['is_duplicate'] = tweets[\"tweet_text\"].duplicated(keep=\"first\")\n",
    "n_duplicates = sum(tweets.is_duplicate)\n",
    "\n",
    "assert n_duplicates == 17717, f'Your function spotted {n_duplicates}, it should have spotted 17717.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCs2ZveuH6jg"
   },
   "source": [
    "We can see the results of the cleanup after you run the following cell. The scatter plot now shows which tweets out of our sample we removed. \\\\\n",
    " _(If you have difficulty distinguishing between the colors used in the plot, please look at the **FAQ** on how to enhance visibility)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x62pN3s2dqOk"
   },
   "outputs": [],
   "source": [
    "# this line combines the two columns into a new one using binary operators\n",
    "# ~ is not, | is or --> keep everything that is not a retweet or a duplicate\n",
    "tweets['to_keep'] = ~ (tweets.is_retweet | tweets.is_duplicate)\n",
    "\n",
    "# hack because plotly has a bug for using bools as coloring attribute\n",
    "tweets['to_keep_str'] = tweets['to_keep'].astype(str)\n",
    "\n",
    "px.scatter(tweets[5000:5050],x='time',y='length',color='to_keep_str',hover_data=['tweet_text'], color_discrete_sequence=px.colors.qualitative.Plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LDd6wchXUBK"
   },
   "source": [
    "### Removing spam accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEKpYCfBX6WF"
   },
   "source": [
    "Let's look at how often people tweeted tweets containing our query phrases over the duration of crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsWVMjejWgKU"
   },
   "outputs": [],
   "source": [
    "# This command counts how often a particular value appears in the column of the dataframe\n",
    "# In this case, we ask how often each username appears\n",
    "counts = tweets.username.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4SvqQWbfLuC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "count_list = list(counts)\n",
    "plt.hist(count_list, log=True, bins=100)\n",
    "plt.xlabel('Number of tweets')\n",
    "plt.ylabel('Number of users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1GCRgDCffXl"
   },
   "source": [
    "Looks like there are some people who tweeted a lot! What do they actually tweet about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vbzVj6beF9O"
   },
   "outputs": [],
   "source": [
    "# List the 10 most frequent usernames\n",
    "print(counts.nlargest(10))\n",
    "\n",
    "# Print some of the tweets of each of the 5 most frequent usernames\n",
    "most_frequent = counts.nlargest(5).to_dict()\n",
    "# for each username in the set\n",
    "for name in most_frequent.keys():\n",
    "  # get a sample of the associated tweets and print\n",
    "  sample = tweets[tweets.username == name].tweet_text.sample(5)\n",
    "  print(name)\n",
    "  print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8d5Fsn-gxUo"
   },
   "source": [
    "Now, where to go from here? Should we drop all these tweets because they're spam? Maybe we should only drop some, based also on what the actual content of the tweets is? Questions like these usually don't have an easy, clear-cut answer. Depending on your research question, you might decide to drop them, especially if you're trying to infer some more general facts about all the people that use Twitter. In contrast, if your analysis is on the level of individual users, finding that they engage in this behaviour might be highly valuable information.\n",
    "\n",
    "For this small study we want to remove all tweets from users who tweeted more than twice in our sample.\n",
    "\n",
    "Task 5: First, make a list of all accounts who tweeted more than twice. Then, using that list, complete the cell below to make a column that identifies tweets from spam accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "on4oVWBJJanc"
   },
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "# Create a list of usernames of all people who exceed the threshold\n",
    "spammers = counts[counts > threshold].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vrt0rSH6gs1l"
   },
   "outputs": [],
   "source": [
    "# Mark their tweets\n",
    "# Hint: Use the isin function from the pandas api on the correct column\n",
    "tweets['is_from_spammer'] = tweets[\"username\"].isin(spammers)\n",
    "\n",
    "n_spam_tweets = sum(tweets.is_from_spammer)\n",
    "assert n_spam_tweets == 15083, f'Your function spotted {n_spam_tweets}, it should have spotted 15083.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7ZkL8BlJrwD"
   },
   "source": [
    "Let's plot our fifty tweets again, this time also flagging up spam tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czEJtIXTrB_Q"
   },
   "outputs": [],
   "source": [
    "tweets['to_keep'] = ~ (tweets.is_retweet | tweets.is_duplicate | tweets.is_from_spammer)\n",
    "\n",
    "# hack because plotly has a bug for using bools as coloring attribute\n",
    "tweets['to_keep_str'] = tweets['to_keep'].astype(str)\n",
    "\n",
    "px.scatter(tweets[5000:5050],x='time',y='length',color='to_keep_str',hover_data=['tweet_text'], color_discrete_sequence=px.colors.qualitative.Plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd_xyhtTqpMU"
   },
   "source": [
    "Q2: Now after the cleanup, look at the fifty tweets again.\n",
    "\n",
    "2.1) Did we manage to increase the percentage of tweets that contain a genuine dream description? \\\\\n",
    "2.2) What problems still remain?\n",
    "\n",
    "A:\n",
    "- 2.1 Yes, about 50% of the data that we keep is about actual dreams that people have had.\n",
    "- 2.2 Most of the rest of the data we keep is about the song \"I Dreamed A Dream\", those data points should be removed. Some of those tweets mention the song by title directly, but others only using quotes from that song, like \"I dreamed that love would never die...\" and \"I dreamed a dream in time gone [by...]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBPUajKV3r9-"
   },
   "outputs": [],
   "source": [
    "# actually drop these instances now\n",
    "tweets = tweets[tweets.to_keep]\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlQpblBrXmeM"
   },
   "source": [
    "### Changing tweet formatting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9Gcm-jaX86D"
   },
   "source": [
    "Let's try and eliminate some of the variation in the tweets that we're not interested in. We'll define functions for replacing usernames, hashtags and links with generic tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhX9KaPkXtgx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# uses a regex to detect usernames and replaces them by USERNAME\n",
    "def replace_username(in_string):\n",
    "    return re.sub('@(\\w){1,15}','USERNAME',in_string)\n",
    "# uses a regex to detect hashtags and replaces them by HASHTAG\n",
    "def replace_hashtag(in_string):\n",
    "    return re.sub('#(\\w)*','HASHTAG',in_string)\n",
    "# uses a regex to detect links and replaces them by LINK\n",
    "def replace_link(in_string):\n",
    "    return re.sub('(http:)?//t.co/\\w*', 'LINK',in_string)\n",
    "\n",
    "# map the text to itself, applying each of the functions\n",
    "tweets.tweet_text = tweets.tweet_text.map(replace_username)\n",
    "tweets.tweet_text = tweets.tweet_text.map(replace_hashtag)\n",
    "tweets.tweet_text = tweets.tweet_text.map(replace_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8tke0WibkJM"
   },
   "source": [
    "Now, let's also remove everything that has a link in it. We'll assume a genuine dream description does not contain a link.\n",
    "\n",
    "Task 6: Make a function that returns true if the new LINK token is in a tweet, then add a new column 'has_link' to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYK4-JTDKgLw"
   },
   "outputs": [],
   "source": [
    "# A function that indicates whether a string is a retweet\n",
    "def hasLink(string):\n",
    "    return \"LINK\" in string\n",
    "\n",
    "tweets['has_link'] = tweets[\"tweet_text\"].map(hasLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1hEQCozKv4i"
   },
   "outputs": [],
   "source": [
    "n_tweets_with_link = sum(tweets.has_link)\n",
    "assert n_tweets_with_link == 5508, f'Your function spotted {n_tweets_with_link}, it should have spotted 5508.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT6gYRCdQ6hl"
   },
   "outputs": [],
   "source": [
    "# drop the tweets\n",
    "tweets = tweets[~tweets.has_link]\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNnII4JcKzOx"
   },
   "source": [
    "If everything went correctly up to here, you'll notice we've dropped close to half of our original tweets! Data cleaning often takes a heavy toll. Now, let's look into the actual text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvpDav2CgUc1"
   },
   "source": [
    "## Text analysis\n",
    "\n",
    "What dreams do people describe in their tweets?\n",
    "\n",
    "Maybe we should first look at what words are frequent to appear in the tweets? For this purpose, we'll need to split them into lists of tokens. A token is a string of characters separated by white space. We thus need to perform tokenization, i.e. we need to split punctuation marks from words.\n",
    "\n",
    "NLTK has a variety of tokenizers. The most commonly used tokenizers is just a function from the base package called *word_tokenize*. However, there is also a tokenizer specifically for tweets, called the tweet tokenizer.\n",
    "\n",
    "This tokenizer has three parameters. For each of them, by means of varying the *sample_sentence* and by checking the documentation, find out what it does.\n",
    "\n",
    "Q3: What is the effect of the three parameters of the TweetTokenizer?\n",
    "\n",
    "A: \n",
    "- The first parameter, `preserve_case` is a boolean that asks us if we want to preserve case in text. Depending on our use case, the difference between uppercase and lowercase may be relevant. However, we pass `false`, and thus are able to automatically convert all text to lowercase (except for emoticons). \n",
    "- The second parameter, `reduce_len`, is a boolean that when true, reduces the length of tokens with characters repeating more than 3 times. It cuts them off at 3. For example, in the scatter plot from earlier in this notebook, there was a tweet that contained a token along the lines of 'byyyyyyyyyyyy.............'. If `reduce_len=True`, then this will be shortened to 'byyy...'.\n",
    "- The third parameter, `strip_handles`, automatically removes Twitter handles from text prior to tokenization if set to True. Twitter handles are the unique names by which a Twitter user may be identified and mentioned in other tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zj2r4cLrjGHM"
   },
   "outputs": [],
   "source": [
    "from nltk import download, FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "# downloading this NLTK data might take a while depending on your connection\n",
    "download('punkt')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPTfwnvzuSvb"
   },
   "outputs": [],
   "source": [
    "sample_sentence = 'We\\'ll need to split them into lists of tokens. And then some more.'\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles= True)\n",
    "token_list = tokenizer.tokenize(sample_sentence)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxgr9T1acEQJ"
   },
   "source": [
    "Now, let's look at the distribution of tokens in the sentence. We'll use the FreqDist class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDLBgKuucCHx"
   },
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(token_list)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KE2wr8tyw9g"
   },
   "source": [
    "Q4: What does invoking the FreqDist result in?\n",
    "\n",
    "A: Invoking the `FreqDist` function on a tokenized text returns an object of type `nltk.probability.FreqDist` that contains the count distribution of tokens of the input, ordered by count. We can see that every token appears only once, except for the period, which occurs twice in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNLrE3xl03Zr"
   },
   "source": [
    "Let's try to only look at a window of words following the query phrase. We made a function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE-fMp0az2RU"
   },
   "outputs": [],
   "source": [
    "def find_following_tokens(tweet):\n",
    "  phrase = tweet.phrase\n",
    "\n",
    "  parts = tweet.tweet_text.lower().split(phrase.lower())\n",
    "  words_after = parts[-1]\n",
    "\n",
    "  token_list = tokenizer.tokenize(words_after)\n",
    "\n",
    "  return token_list\n",
    "\n",
    "tweets['immediately_after'] = tweets.apply(find_following_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrutqgUmOcbc"
   },
   "outputs": [],
   "source": [
    "tweets['immediately_after'].sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj3-58GWceUY"
   },
   "source": [
    "Let's look at the distribution of words in this window for every tweet. Run the following two cells to get a glimpse of the distribution in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgDJhNySjXVv"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "pool = Pool(cpu_count())\n",
    "out = pool.map(FreqDist, tweets.immediately_after.to_list())\n",
    "pool.close()\n",
    "\n",
    "word_dist = FreqDist()\n",
    "for dist in out:\n",
    "  word_dist.update(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUejichzxXPX"
   },
   "outputs": [],
   "source": [
    "word_freqs = sorted(word_dist.values(),reverse=True)\n",
    "plt.plot(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuQ8XgDt4YLB"
   },
   "source": [
    "Q5:\n",
    "\n",
    "5.1) How should the x and the y axis be labelled? \\\\\n",
    "5.2) What pattern can you see in the plot? \\\\\n",
    "5.3) Referring back to the lecture slides, do you know a name for this phenomenon?\n",
    "\n",
    "A: \n",
    "- 5.1 In this plot, the X axis is an ID of a token, and the Y axis is the count of that token appearing in the tweets. This plot is showing the amount of times that a token appears in the tweets.\n",
    "- 5.2 What we can see is that very few tokens occur many times, while many, many tokens occur very few times, if not only once.\n",
    "- 5.3 This is called Zipf's law: the amount of times a token occurs in a text is 1 over its word rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0gkXgyvj8cO"
   },
   "source": [
    "Now, let's look at some of the words to see if we can identify any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE5T0HaEj8Rm"
   },
   "outputs": [],
   "source": [
    "word_dist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws3_eujEyqp-"
   },
   "source": [
    "Nothing too remarkable, right?\n",
    "\n",
    "Instead of only looking at single words at a time, let's try looking at longer strings of words. Below, we have specified a function for your convenience. It extracts n-grams, sequences of n adjacent words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ECfPERZBETr"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def extract_ngram_freqs(token_list, n):\n",
    "  grams = list(ngrams(token_list, n))\n",
    "  cleaned_grams = []\n",
    "  for word_tuple in grams:\n",
    "      for word in word_tuple:\n",
    "          if word not in stopwords:\n",
    "              cleaned_grams.append(word_tuple)\n",
    "              break\n",
    "\n",
    "  return cleaned_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLLX7sfDY5Ec"
   },
   "outputs": [],
   "source": [
    "extract_ngram_freqs(['I', 'dreamed', 'about', 'you', 'and', 'you', 'were', 'beautiful'], n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQPUmTFmYzja"
   },
   "source": [
    "Q6: Run the cell above and vary the parameters of the function call, i.e. by feeding in another short sentence in the same format or by changing n.\n",
    "\n",
    "6.1) What does the function do? \\\\\n",
    "6.2) What exactly is extracted? \\\\\n",
    "6.3) What exactly is omitted, i.e. left out? (Yes some things are omitted!)\n",
    "\n",
    "A: \n",
    "- 6.1 This function returns n-grams for a given token list and value of n, except for n-grams that exclusively consist of stopwords.\n",
    "- 6.2 We get all n-grams that may be constructed for the given token list and value of n, except for n-grams that solely consist of stopwords.\n",
    "- 6.3 We omit n-grams that exclusively consist of stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnHk5HpgZMgG"
   },
   "source": [
    "To extract n-grams for a large number of tweets, we've specified another function that uses multiprocessing. You don't need to understand its detailed workings, but you can treat it as a prototype in case you ever need to parallelise an extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4pB_ajXDJgt"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "'''\n",
    "A function to extract ngrams for every tweet in a frame.\n",
    "If not present, adds a column with the list of n-grams to the frame.\n",
    "Its name is dependent on n.\n",
    "'''\n",
    "def extract_and_add_ngrams(frame, token_column, n):\n",
    "  token_list = tweets.immediately_after.tolist()\n",
    "  list_of_grams = []\n",
    "  \n",
    "  for tokens in token_list:\n",
    "    gram = extract_ngram_freqs(tokens, n)\n",
    "    list_of_grams.append(gram)\n",
    "\n",
    "  # so grams1, grams2 and so on\n",
    "  name = 'grams' + str(n)\n",
    "  if name not in frame.columns:\n",
    "    tweets[name] = list_of_grams\n",
    "  return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oJlS0YbZxNu"
   },
   "source": [
    "Let's try and graphically inspect these n-grams. We'll download a package called wordcloud and then make some word clouds for different settings of n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AjtzXFIR9vU"
   },
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdvnMtboZ_RL"
   },
   "source": [
    "This is a helper function that takes care of the plotting. You don't need to understand what it does to proceed with the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oi_DfBD0lob"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import MLEProbDist\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dist_as_cloud(word_dist):\n",
    "  prob_dist = MLEProbDist(word_dist)\n",
    "  viz_dict = {}\n",
    "  for word_tuple in word_dist:\n",
    "    string = ' '.join(word_tuple)\n",
    "    viz_dict[string] = prob_dist.prob(word_tuple)\n",
    "\n",
    "  cloud = WordCloud(width=1600,height=400).generate_from_frequencies(viz_dict)\n",
    "\n",
    "  plt.figure(figsize = (25,25))\n",
    "  plt.imshow(cloud, interpolation='bilinear')\n",
    "\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy3xaIaUagHP"
   },
   "source": [
    "The following cell extracts and plots n-grams. Vary the parameter, trying out some settings between say 1 and 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGnrWFRwJqRN"
   },
   "outputs": [],
   "source": [
    "# Vary this parameter\n",
    "# See FAQ in case of function not terminating\n",
    "n = 5\n",
    "# extracts the n-grams\n",
    "column_name = extract_and_add_ngrams(tweets,'immediately_after', n = n)\n",
    "\n",
    "word_dist = FreqDist()\n",
    "for grams in tweets[column_name]:\n",
    "  word_dist.update(FreqDist(grams))\n",
    "print(f'Found {len(word_dist)} unique n-grams for n = {n}')\n",
    "\n",
    "most_common = word_dist.most_common(25)\n",
    "for word_tuple in most_common:\n",
    "  print(word_tuple)\n",
    "# calls our helper function for plotting\n",
    "plot_dist_as_cloud(word_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nStGzQ0xawdh"
   },
   "source": [
    "Q7: \\\\\n",
    "What happens when you increase the size of n from 2 to 5? Specifically, \\\\\n",
    "7.1) What happens to the number of n-grams? \\\\\n",
    "7.2) What happens to the occurrence count of the most frequent n-gram? \\\\\n",
    "7.3) Why do you think this is the case?\n",
    "\n",
    "A: \n",
    "- 7.1 The number of n-grams increased significantly when the n went from 2 to 5. Specifically, n = 2 resulted in 181.467 grams and n = 5 resulted in 428.253 grams.\n",
    "- 7.2 It decreased significantly, from 5100 with n = 2 to 220 with n = 5.\n",
    "- 7.3 When we increase n, we increase the amount of possible unique grams we can generate. This decreases the chance that the same gram occurs multiple times, and so our occurrence count of the most frequent n-grams decreases too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI0ZCfzEdpii"
   },
   "source": [
    "Both the most frequent word n-grams and the word cloud remarkably show that certain exact phrases are repeated by many users. We would not expect that users express their  personal dream experiences in the exact same words, and indeed these tweets do not express dreams but something else.\n",
    "\n",
    "Q8: So what is going on here? Why are people using the same phrase over and over?\n",
    "\n",
    "A: People are quoting the song \"I Dreamed a Dream\" from the musical \"Lés Miserables\". The song opens with the sentence \"I dreamed a dream in time gone by\". It is likely that many enjoyers of Les Miserables, and/or the song itself, quote the opening of the song to express that they enjoy it. This is why, for example, when n = 5, the n-gram with the highest count is an incomplete version of the opening lyric of I Dreamed a Dream (\"[I dreamed] a dream in time gone [by]\")\n",
    "\n",
    "Q9: Can you think of ways to remove these tweets from the sample ? (you do not need to code this solution, only reflect on the problem)\n",
    "\n",
    "A: One way to remove these tweets, is to remove all tweets that contain a specific sentence also present in I Dreamed a Dream. Although that is assuming that everyone who says this phrase is referencing the song, since the lyric itself is a relatively very long piece of text, it will be very likely that practically anyone using it will be referencing the song.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFTV5IJENwY2"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# give people the chance to try to find which tweets contain these grams\n",
    "\n",
    "def has_phrase(gram_list, phrase_tuple):\n",
    "  return str(phrase_tuple in gram_list)\n",
    "\n",
    "def search_for_phrase(frame, phrase):\n",
    "\n",
    "  phrase = phrase.lower()\n",
    "  phrase_parts = phrase.split(' ')\n",
    "  phrase_tuple = tuple(phrase_parts)\n",
    "\n",
    "  search_func = partial(has_phrase,phrase_tuple=phrase_tuple)\n",
    "\n",
    "  search_field_name = 'grams' + str(len(phrase_tuple))\n",
    "  if search_field_name not in frame:\n",
    "     extract_and_add_ngrams(frame,'immediately_after',len(phrase_tuple))\n",
    "\n",
    "  search_frame = frame.copy()\n",
    "  search_frame['match'] = search_frame[search_field_name].apply(search_func)\n",
    "  return search_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8tJp7X7kJPx"
   },
   "source": [
    "Maybe one of those phrases has piqued your interest and you want to look at some of the tweets that contain it? In the following cell, you can swap out the contents of the phrase variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFDh5mMskC6Y"
   },
   "outputs": [],
   "source": [
    "phrase = 'about you nearly every night'\n",
    "search_frame = search_for_phrase(tweets,phrase)\n",
    "px.scatter(search_frame,x='time',y='length',color='match',hover_data=['tweet_text'], color_discrete_sequence=px.colors.qualitative.Plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "almbJsw9faTr"
   },
   "source": [
    "Q10: Try searching for the phrase 'about you nearly every night'. Can you make a guess where people found the inspiration for that phrase?\n",
    "\n",
    "A: The inspiration for most tweets is likely to be the song \"Do I Wanna Know?\" by Arctic Monkeys. This song was released in 2013, but reached a peak in popularity around March of 2014, [where it reached number 70 in the US Billboard Hot 100 charts](https://www.billboard.com/charts/hot-100/2014-03-22/), the highest noted score it achieved in the list. Since our dataset consists of tweets from the period of June to August of 2014, it is likely that for many people, this song would still be on their mind frequently, and that would compel them to tweet about it, citing its lyrics. One of those lyrics contain the phrase \"I've dreamt\". What is likely to have happened, is that the people tweeting these tweets interpreted the lyric \"I've dreamt\" as \"I dreamed\" and tweeted the erroneously altered version of the lyric \"I dreamed about you nearly every night this week\". These alterations were crawled up during our data collection process and are now part of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXBH6gbJrV3N"
   },
   "source": [
    "## Discussion ##\n",
    "\n",
    "Our starting point for this study was to investigate what the dream themes most frequently described on Twitter are. We inspected the data sample and attempted various cleaning steps.\n",
    "\n",
    "However: your inspection of the data has uncovered a huge problem with the sample as it turns out to be extremely difficult to separate genuine dream reports from other tweets that for some other reason contained the phrase 'I dreamed'.\n",
    "\n",
    "Q11: Do you think that the data sample we are using is actually suited to answer your research question 'What are the dream themes most frequently described on Twitter'? Explain your answer.\n",
    "\n",
    "A: I would not say that our dataset is completely ill-suited for our research question. The dataset contains data that are actually about dreams that people have dreamt. However, it is clear that only a fraction of the dataset actually consists of such data. There is much topically unrelated data in the dataset that are not about actual dreams. I think that potentially, tweets that were crawled under stricter terms could make for a better suited dataset for our research question. For example, I suggest we might add the requirement that the tweet contains the phrase \"last night\" or \"tonight\" alongside \"I dreamed\" and \"I have dreamt\", maybe even only as concatenations of one another. While this certainly is no bulletproof solution, I suspect that it will result in a dataset with less unrelated tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_tBjGGeoGwK"
   },
   "source": [
    "## Dream themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRe4zy7nf_Rp"
   },
   "source": [
    "So far, our bottom-up approach to dream data on Twitter has revealed little. Given the high number of tweets and the strong noise in the data, manual inspection for dominant topics is quite hard. We could try and ease this process, e.g. by attempting topic modelling or some dimensionality reduction technique, but these topics will only be discussed in the lectures over the coming weeks.\n",
    "\n",
    "For now, let's try another way. Let's make up an explicit hypothesis about what people dream about based on a [scientific study](https://dreams.ucsc.edu/Library/bulkeley_2010.html) of dream reports by Bulkeley, K., and Domhoff, G. W. (2010), that counted how certain themes are mentioned in dream reports by simply counting words.\n",
    "\n",
    "Two common themes are flying and falling. We'll then try and separate our tweets based on whether they actually contain words associated with them. Let's see how frequent these themes really are in our Twitter sample!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVWsz4GLkUeV"
   },
   "source": [
    "We made a dictionary whose keys represent the two themes. For each key, there is  a list of associated phrases as used in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn2lYF8sEZjX"
   },
   "outputs": [],
   "source": [
    "dream_class_dict = {\n",
    "    'falling': ['falls','fell','falling','collapses','collapsed','collapsing','drops','dropped','dropping'],\n",
    "    'flying': ['fly','flies','flew','flying','floats','floated','floating','glides','gliding','glided']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br-0lVVtyfqu"
   },
   "source": [
    "The following code cells extract the relevant word counts from the tweets and then classify each tweet based on the match with the word lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izZ9GGhVhVEs"
   },
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "n_grams_needed = set()\n",
    "for dream_class in dream_class_dict.keys():\n",
    "    list_of_phrases = dream_class_dict[dream_class]\n",
    "    grams = []\n",
    "    for phrase in list_of_phrases:\n",
    "      phrase = phrase.lower()\n",
    "      words = phrase.split(' ')\n",
    "      gram_rep = tuple(words)\n",
    "      n_grams_needed.add(len(gram_rep))\n",
    "      grams.append(gram_rep)\n",
    "    new_dict[dream_class] = grams\n",
    "\n",
    "for n_gram in n_grams_needed:\n",
    "   list_name = 'grams' + str(n_gram)\n",
    "   if list_name not in tweets.columns:\n",
    "     extract_and_add_ngrams(tweets,'immediately_after',n_gram)\n",
    "\n",
    "dream_class_dict = new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vh1QZpmhaWn"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Classify a tweet according to its dream class. Returns the class name of the best match.\n",
    "Unassigned is returned if no matching class was found.\n",
    "'''\n",
    "def classify_tweet(tweet):\n",
    "  class_scores = {}\n",
    "  for dream_class in dream_class_dict.keys():\n",
    "    gram_list = dream_class_dict[dream_class]\n",
    "    counter = 0\n",
    "    for gram in gram_list:\n",
    "      # get the correct gram list for the lookup\n",
    "      list_name = 'grams' + str(len(gram))\n",
    "      grams_in_tweet = tweet[list_name]\n",
    "      if gram in grams_in_tweet:\n",
    "        counter = counter + 1\n",
    "    class_scores[dream_class] = counter\n",
    "\n",
    "  highest_score = 0\n",
    "  best = 'unassigned'\n",
    "\n",
    "  for class_name in class_scores.keys():\n",
    "    if class_scores[class_name] > highest_score:\n",
    "      highest_score = class_scores[class_name]\n",
    "      best = class_name\n",
    "  return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7asLmoMyvjY"
   },
   "source": [
    "Now, let's plot the tweets again, using the assigned classes as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVTaNlGWRTAj"
   },
   "outputs": [],
   "source": [
    "class_frame = tweets.copy()\n",
    "class_frame['class'] = tweets.apply(classify_tweet, axis=1)\n",
    "\n",
    "px.scatter(class_frame,x='time',y='length',color='class',hover_data=['tweet_text'], color_discrete_sequence=px.colors.qualitative.Plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97laRNdyitx7"
   },
   "source": [
    "Q12: In the Bulkeley & Domhoff study sourced above, falling dreams are reported twice as often as flying dreams. Is this the same in the Twitter sample? Explain how you derived your answer.\n",
    "\n",
    "A: I would not say that that is also the case in our Twitter sample. While it does seem that tweets about flying are sparser than dreams about falling, it doesn't quite seem to be double. I expect that this is because a fair number of tweets still aren't about actual dreams, but something else. For those tweets, the popularity of words about flying and falling affect the distribution of our dream themes.\n",
    "\n",
    "Q13: What limitations do you see in this paradigm? Try to make sense of the classify_tweet function.\n",
    "\n",
    "13.1) How does it work? \\\\\n",
    "13.2) Do you see errors it makes? \\\\\n",
    "13.3) How could it be improved?\n",
    "\n",
    "A: \n",
    "- 13.1 The `classify_tweet` function works by enumerating the number of times a string in the flying or falling lists of the dream themes dictionary appears in the tweet. The tweet then gets assigned a theme based on the highest count of words of a dream theme found in the tweet.\n",
    "- 13.2 If the counts are equal, that is to say, if the number of words that identify a tweet as a \"flying\" dream are the same as the number of words that identify the twee as a \"falling\" dream, then tweet only get assigned as one of the two themes, not both. Furthermore, if no words were found, then the twee is \"unassigned\", despite possibly still being about flying or falling, e.g. by using a synonym.\n",
    "- 13.3 A \"flying & falling\" or \"both\" class could be introduced for dreams that are both about flying and falling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14DyxirsJn9H"
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Here you can find a list of frequently asked questions. Make sure to check them out!\n",
    "\n",
    "**Q: Will I get a grade for this assignment?** \\\\\n",
    "A: All assignments will be graded with a Pass or a Fail. You need to pass all assignments in order to take the exam. Make sure to checkout the learning objectives for each assignment, as the assessment will be based on whether we think you've passed the learning objectives.\n",
    "\n",
    "**Q: What if I don't know the answer to a task or question?** \\\\\n",
    "A: First of all, don't worry! You can contact the TAs and ask for help during the office hours on Monday (physically or through discord), or by sending an email.\n",
    "\n",
    "**Q: I get the output message 'Not all tweets were loaded', what am I doing wrong?** \\\\\n",
    "A: Sometimes after reloading the notebook in Colab, not the entire data file is uploaded. Make sure to check whether it is correctly uploaded, and if needed reupload. Another possibility is that the file contains a character that for some reason is not handled correctly on your computer. This doesn't happen often, but you can check at which tweet your load_data function stopped parsing and manually inspect the tweet to remove/alter the character.\n",
    "\n",
    "**Q: I think I did a task correctly, but the assert tells me I don't have the correct number of tweets left, what am I doing wrong?** \\\\\n",
    "A: Most likely, you have a code that indeed works properly, but that does not satisfy the exact given requirements of the task. Please reread the task and check whether you satisfy the requirements exactly as proposed.\n",
    "\n",
    "**Q: I have strange bugs in the base code you provided, what do I do?** \\\\\n",
    "A: This most likely has to do with settings specific to your local machine and/or python/library versions. All our code has been implemented and tested using [Google Colab](https://https://colab.research.google.com/), because this is relatively hassle-free. Please consider switching in case you experience such problems. We can try to help you out on other platforms, but cannot guarantee that we can solve it. The problems should be solved automatically when switching to Colab!\n",
    "\n",
    "**Q: Plotly 4.0.0 won't install on my local machine, how can I fix this?** \\\\\n",
    "A: In case you're running the assignment locally and you get the error that it could not find a version that satisfies the requirement, try removing the apostrophes around 4.0.0. Another solution may be to first run 'uninstall plotly' and then run the code to install plotly 4.0.0.\n",
    "\n",
    "**Q: The provided code for extract_and_add_ngrams is raising an error, how can I solve this?** \\\\\n",
    "A: Sometimes, the function extract_and_add_ngrams raises errors on local machines due to the use of multiprocessing. Switching to Colab should automatically solve the issue without changing the code. In case you do not want to do so, you can replace the given function with the following code:\n",
    "\n",
    "```\n",
    "def extract_and_add_ngrams(frame, token_column, n):\n",
    "  token_list = tweets.immediately_after.tolist()\n",
    "  list_of_grams = []\n",
    "  for tokens in token_list:\n",
    "    gram = extract_ngram_freqs(tokens, n)\n",
    "    list_of_grams.append(gram)\n",
    "\n",
    "  # so grams1, grams2 and so on\n",
    "  name = 'grams' + str(n)\n",
    "  if name not in frame.columns:\n",
    "    tweets[name] = list_of_grams\n",
    "  return name\n",
    "```\n",
    "\n",
    "**Q: The function for extracting and plotting n-grams runs infinitely and never terminates with output, what do I do?** \\\\\n",
    "A: Sometimes, running on your local machine may result in this. In the past, a reason has been computing power. Try lowering the parameter n. In case this does not work, please consider moving to Colab.\n",
    "\n",
    "**Q: I get a different number of tweets every time I load the data, what is happening?** \\\\\n",
    "A: Remember to always close a file after you are finished reading it.\n",
    "\n",
    "**Q: My function for duplicate detection detects 17717 duplicates, what am I missing?** \\\\\n",
    "A: Most likely you have not removed whitespaces at the end of the tweet when loading the data.\n",
    "\n",
    "**Q: I cannot see the difference between the coloured dots in the plots. How can I enhance visibility?** \\\\\n",
    "A: In the scatter plots called with `px.scatter(...)` you can update the `color_discrete_sequence` parameter to a color sequence with more contrast. A list of possible color sequences is given [here](https://plotly.com/python/discrete-color/#color-sequences-in-plotly-express)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
